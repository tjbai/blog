{
  "emails": {
    "-NMCcsm2DLu9Z7MLGsxc": {
      "email": "tbai4@jh.edu",
      "who": "me"
    },
    "-NMCjMtUH8i94sUmA7T0": {
      "email": "tianningbai@gmail.com",
      "who": "tianning"
    },
    "-NMEI8EjuAg-z2zkZr0P": {
      "email": "v.dong0108@gmail.com",
      "who": "viv :)"
    },
    "-NMgJ4Xb77iSf0WtGrOw": {
      "email": "hcho64@jhu.edu",
      "who": "lil bro"
    },
    "-NUnV_5E03-DmvMhbHrZ": {
      "email": "ztung1@jh.edu",
      "who": "Zachary"
    },
    "-NUnVbF80frPDj4WWl1x": {
      "email": "taran.agarwal@gmail.com",
      "who": "Taran"
    },
    "-NjRHPoIREJqMooeRA2X": {
      "email": "avnukala@gmail.com",
      "who": "jit"
    },
    "-NmntkGIKuflc22XCM6c": {
      "email": "txie119@gmail.com",
      "who": "Theo"
    },
    "-O4GV0_pPhi-amv_CK_S": {
      "email": "",
      "who": "nn"
    },
    "-O61fYcqJ1Tj2zTHCmSk": {
      "email": "licli019@gmail.com",
      "who": "Chris Li"
    }
  },
  "published_posts": {
    "-NMBxkm2dEdE_myIqP4W": {
      "body": "So the natural question to be asking at this point is, \"what the fuck is this?\" \n<br></br>\n<br></br>\n\nThat's a great question because, to be honest, I don't really know. In the next XXX words I will try my very best to rationalize away the time I spent building this website in a disorganized, hasty format.\n<br></br>\n<br></br>\n\np.s. Skip the first few points if you don't wanna read the sappy stuff.\n<br></br>\n<br></br>\n\n$ #1. I'm a productive procrastinator.\n\nProbably the best reason. There's about a dozen things I should be doing right now that would be a productive use of my time. Unfortunately, those things are boring and scary so I told myself that writing some code and shitty blog posts could be educational and formative.\n\n<br></br>\n<br></br>\n\nSomehow, I believed the lie I told myself and so here we are.\n\n<br></br>\n<br></br>\n\n$ #2. I'm a terrible writer.\n\nIn 2020, I made a New Year's resolution to read 12 books—1 for each month. Over the course of 7 month, I ended up burning through close to 20 (before I fell off the bus when college started) which sums to more literature than I had likely read in the previous decade. A vast majority of the reading was boring and forced, yet, I found great satisfaction in following through with my goal and making it through some behemoths like *The Count of Monte Cristo* and *War and Peace*.\n\n<br></br>\n<br></br>\n\n2 years later, I confidently believe that I've retained a level of insight and ancillary knowledge from that brief reading stint that benefits me to this day—all for the simple cost of spending a tiny percentage of my day reading some really fking old books. \n\n<br></br>\n<br></br>\n\nWith that experience behind me, I am now making the late 2022 New Year's resolution to write about 3-4 of these shitty blog posts each week. I fully anticipate that a lot of those posts will be terrible and far less motivated than this pilot, but I also find it hard to believe that I'll walk away from this worse than I started out.\n\n<br></br>\n<br></br>\n\n$ #3. I narcissistically believe some of the things floating in my head are worth sharing.\n\nI spend a lot of my time feverishly consuming media from various podcasts, Substacks, blogs, and youtube channels. A lot of the time, the content is pure garbage. BUT, every once in a while I come across an article, or a podcast episode, or even a few sentences which make it worth it. These posts are my way of panning for gold nuggets.\n\n<br></br>\n<br></br>\n\n$ #4. Building things are fun and this is my guinea pig.\n\nYou've probably noticed by now (if I sent you this link) that \"[domain i end up buying]\" *isn't* a Substack, or Medium, or Notion, or whatever the fk. If my motives were purely focused on self-improvement yadayada, there's no real reason I would or *should* have spent the last few hours building this site out. But, making my own site has a lot of perks, on top of simply being fun to spin up.\n\n<br></br>\n<br></br>\n\nI might dedicate an upcoming post to some of the things that I've already implemented that are kinda cool (my own markdown language being the biggest one), and all the things I still want to mess around with. I full intend for this website's codebase to be a living, breathing behemoth / a frankenstein monster of various technologies and extensions, but to not bore some of you just look at this:\n\n$spin/40/Look at me\n\n<br></br>\n<br></br>\n\nThis is a pretty simple animation if you know your CSS, but the perk is that in my editor (which you can try getting access to through the frontpage) all I have to type is `$spin/40/Look at me`. It's a contrived example, but I do genuinely have some cool (I think I'm abusing that word) ideas on how to embed slick stuff into these posts and this website as a whole.\n\n<br></br>\n<br></br>\n\nLiterally *anything* I want to do, I know that I can probably do—that's a pretty nice feeling.\n\n<br></br>\n<br></br>\n\n$ Conclusion\n\nI honestly thought I'd have a few more things to write about but I'll probably end up saving a lot of material for future posts since I know I'll run out fast. Before this wraps, a few notes for the 0-2 users I might get in the long-run:\n\n<br></br>\n<br></br>\n- Published posts are untouchable. I'm way too lazy to go into Firebase and edit the markup in a GUI so you know I'm not lying. Typos, grammatical errors, and bad writing will all be apart of the public record *forever*, or at least until Google Cloud implodes.\n- Things will break a lot.\n- Some features will taunt you if you try to mess with them. Could I have made the UI responsive to whether you're logged in or not? Yea sure, but there's no fun in that. Plus, this way when I implement new things *you* get the added inconvenience if figuring out if you're allowed to use it or not. mwaha.\n- I haven't implemented autosaving yet so I can't wait to pour my heart into a blog post just to refresh and have it disappear.\n\n<br></br>\n<br></br>\n<br></br>\n<br></br>\n",
      "createdDate": "2023-01-20T02:17:00.751Z",
      "labels": [
        "Meta"
      ],
      "state": "published",
      "title": "Pilot"
    },
    "-NMbBUMlxgfc54EDc_2G": {
      "body": "I'm not even gonna proofread this one zzz. Writing this post during my Computer Ethics class. Snooze fest. (Does that make me a bad person?)\n<br></br>\n<br></br>\n\nI've decided to stop calling these \"blog posts\" and instead just \"posts\". The former puts a little too much pressure on me to actually produce semi-quality content. I'm a huge fan of this substack called \"The Diff\" by Byrne Hobart. His shit is brilliant and he posts literally *everyday*. To this point I've been trying to hold myself to his standards but I don't have a fraction of his chops. What this means is that moving forward a lot of these *posts* will probably just end up being stream-of-consciousness meditations from my day.\n\n<br></br>\n<br></br>\n\nHowever, that's not to say I won't try to put out somewhat serious content every once in a while too. I have a few drafts going right now which you can try taking a look at. mwaha.\n\n<br></br>\n<br></br>\n\n$ Physics students are brilliant\n\nRegistered for a 400-level quantum computing class hosted in the physics department this semester on a whim (not on a whim, I actually felt guilty that I was only taking 14 credits). The course description said all I needed was some linear algebra, probability, and calculus. \"Gee whiz,\" I thought to myself, \"I know all of those.\"\n\n<br></br>\n<br></br>\n\nI walk into the classroom a few minutes late after getting lost in Bloomberg, and *this shit is packed to the gills*. I end up sitting in the very back of the classroom and don't even get a surface to write on. I pull out my laptop in shame and fire up a new file in Obsidian.\n\n<br></br>\n<br></br>\n\nThe class is co-taught by these 2 staff scientists at the JHU APL who give me strong Leonard and Raj vibes from *The Big Bang Theory*. Leonard (we'll call him) starts giving a talk on the state modern quantum computing and an overview of the theory we'll be covering in the class + expected prerequisite knowledge (spoiler: I'm totally unqualified). The entire time I can't help by notice how different the environment in that classroom is, at least compared to all the upper-level AMS and CS classes I've taken over the last few semesters.\n\n<br></br>\n<br></br>\n\nEvery single set of eyes in that packed room is fully locked on the board, and you can just tell each person *wants* to be there. These aren't your typical CS/AMS majors who just want to get a cushy tech or finance job. These are kids who *really* love what they study and also *totally know their shit*. Each time Leonard pauses for questions, a flurry of hands go up in the air asking some obscure stuff. Here are a few of my favorites...\n\n<br></br>\n<br></br>\n\n$\"Over the summer I ran a few deep learning experiments on IBM Q with just 6 qubits and it was painfully slow, but you said in slide 14 that the state-of-the-art model now might have over 400? What gives?\"\n\n$\"What sociopolitical factors have been primarily driving increased investment in quantum resources over the last few years?\"\n\n$\"What correlation, if any, exists between qubit count and quantum noise? How are we looking right now as an industry with respect to quantum error avoidance and correction?\"\n\nHonestly, pretty tame, but things only proceeded to get hairier when Raj got up to give \"a simple lecture on some basic quantum mechanics you should all be familiar with.\"\n\n<br></br>\n<br></br>\n\nI proceed to have my peanut brain continuously crunched over the next hour with some unexplainable shit (something, hermitians, something, Dirac bra-ket notation, something, tensor products). By no means do I consider myself good or even well-versed in math, but I've at least seen my way through some pretty rigorous courses. Imagine the panic in my head when Raj started to turn one of my favorite subjects (linear algebra) into utter nonsense.\n\n<br></br>\n<br></br>\n\nMeanwhile, the kids around me are absolutely eating it up.\n\n<br></br>\n<br></br>\n\n$\"This is just semantics, but could you clarify on that point about Pauli matrices? Do you mean to say a tensor product of sheaves of modules?\"\n\n$\"Can you expand a bit more on how the angular forms extend to complex numbers? I'm getting the intutition behind normalization and the Bloch sphere but not quite how it works with the conjugate transpose.\"\n\nHalf the class also inexplicably had a british accent which made them about 10x more intimidating. I literally saw 2 kids sitting next to me get visually excited when Raj said he was \"now going to move onto a quick section on entanglement.\"\n\n<br></br>\n<br></br>\n\nBottom line: physics mfs are just a little different. I'll probably end up dropping the class if things continue to trend in this direction, but  I'm definitely motivated to go back at least a few more times.\n\n<br></br>\n<br></br>\n\n$ Old professors\n\nWas pretty hungover at my set theory class this morning (my first 9 am since freshman fall, yuck). Definitely didn't help that the classroom was tiny and the tables were unexplainably sticky. Note to self: kinda hard to think about the Continuum hypothesis and consistency of Zermelo-Fraenkl set theory when God is punishing me for a night of Natty Boh's and Burnett's.\n\n<br></br>\n<br></br>\n\nProf. Rynasiewicz (rye-nuh-shev-its) is a badass mfer. This guy is fighting life one day at a time with an aeropostale hoodie and a 2004 MacBook Pro.  \n\n<br></br>\n<br></br>\n\n$ Miscellaneous Thoughts\n\nClass is wrapping up so just a few more things that I've been working on / have crossed my mind in the last few seconds / hours / days.\n\n<br></br>\n<br></br>\n\n- A quote from my newly-hired Intro Algorithms professor.\n\n<br></br>\n<br></br>\n\n$\"So why should you even come to class? You can find easily all this material online... MIT even posts all their lectures publicly. Yes, MIT! They didn't want me you know, they didn't even shortlist me for a teaching position... (nervous chuckles around the room). Hey! They didn't want any of you guys either (laughs all around).\"\n\n- Making this website is fun because I don't have to worry about making things scale. Stuff can break. Code can be slow. I can be wasteful with my DB calls. None if it matters because no one uses it :).\n<br></br>\n<br></br>\n- I'm still ironing out quite a few issues with the implementation. I keep getting this really weird string parsing error in my markdown code because I don't understand how one of my Chakra UI dependencies works...I tried to put a couple hyperlinks in this post and it just blew everything up and deleted like 15 minutes of writing :P.\n<br></br>\n<br></br>\n- Everyone loves productivity tools. There's literally dozens of million and billion dollar products in this space and at a certain point you've just gotta wonder what the event horizon looks like for all of them. Not to say that these valuations are *wholly* unjustified, but I just don't really buy that \"X hours saved\" is going to be a reasonable measure of worth forever.\n<br></br>\n<br></br>\n- I've been messing around with Obsidian (I know, I know) the last few days since Notion gives me the ick but Stickies aren't really a scalable note system. I've been liking it so far and it definitely makes taking Algos/CSF notes way easier. I'm also just a fan of the IDE-like interface (reminds me a lot of IntelliJ and tons of extensibility) + the knowledge graph stuff. Big graph guy. \n<br></br>\n<br></br>\n- Taking a startup seriously really is no joke 🫠.\n<br></br>\n<br></br>\n\nThat's all for today. It's pretty nuts how much I was able to get done during a single comp ethics class. \n<br></br>\n<br></br>\n<br></br>\n<br></br>\n",
      "createdDate": "2023-01-25T04:31:29.176Z",
      "labels": [
        "Meta",
        "School"
      ],
      "state": "published",
      "title": "Bored in Computer Ethics"
    },
    "-NMugpFgIBV5IGtbgCqM": {
      "body": "Some thoughts after a long night out.\n<br></br>\n<br></br>\n\n$ Dead hype\n\nThere's an absurd amount of bumble these days around generative AI, foundation models, LLMs, etc. with a lot of hive-minders saying that you should / could be getting on the wave to the future by adopting early. \n<br></br>\n<br></br>\n\nSpoiler alert: the wave has already passed (at least for zero and first-order applications).\n<br></br>\n<br></br>\n\nTo hop on now and think you're going to make it big is like buying GME at 500 or saying that Frank Ocean is your favorite \"indie\" artist—the alpha is mostly gone, at least in the obvious ways.\n<br></br>\n<br></br>\n\nTrying to get in with courtside seats after the game has already been played requires an intense amount of expertise, long-term vision, and contrarian perspective that can separate you from the pack—and just like in poker, or venture capital, or public equities, the pack is usually made up of the losers.\n<br></br>\n<br></br>\n\nWith such an insane amount of talent and support in modern tech, the market for startups has become insanely efficient—this makes for great thematic investing, but for the average 19-year old it's no longer the type of world where you can put gifs on the internet and make a billion dollars (or whatever the contemporary AI-equivalent of that might be).\n<br></br>\n<br></br>\n\nLooking forward, I'm most curious to see how second and even third-order (thematic) applications will shape out (there's a future post somewhere here about how this relates to the moments of a probability distribution), and perhaps the fact that someone with as little of an information advantage as myself is thinking this way indicates that it's already been \"priced-in.\" \n<br></br>\n<br></br>\n\nBottom line: I'm skeptical of a lot that's going on right now, and it also makes me a little sad that it'll just become increasingly more difficult to squeak out some alpha for past waves that I'm just a bit more qualified to pursue (Cloud, Mobile, Web 3, etc). Perhaps that's just the way things are.\n<br></br>\n<br></br>\n\n$l$An interesting read if you're a salty old man like me.$https://luttig.substack.com/p/is-ai-the-new-crypto?ref=tj$\n\n<br></br>\n<br></br>\n\n$ Other thoughts\n\n- I've been trying to write a post about all the shit that I've learned over the last couple months of thinking like a founder / CTO but it's not really coming together. Maybe that just means I haven't learned jack.\n<br></br>\n<br></br>\n- Hair of the Dog can not possibly be a profitable business.\n<br></br>\n<br></br>\n- \"God bless Dick Cheney's America\" is one hell of a fking quote. Watch _War Dogs_ if you like good movies, Jonah Hill, or guns.\n\n<br></br>\n<br></br>\n\n$ Some good house music you will appreciate if you're not deaf\n\n$s$https://open.spotify.com/track/5X78zXRKBpf6089Y348Wne?si=eddbee3da0894141$d\n\n$s$https://open.spotify.com/track/6Sblo8WSABiwrqRQLGrOqO?si=3ff510d1648a493f$d\n\n$s$https://open.spotify.com/track/5N6BhjTwtWa4keibdHs45q?si=71939eeb84f84fb9$d\n\n$s$https://open.spotify.com/track/4I58pzQEyFpHLXyDGekkCc?si=dc3e9c70428b47cf$d\n\n<br></br>\n<br></br>\n<br></br>\n<br></br>\n",
      "createdDate": "2023-01-28T19:24:21.090Z",
      "labels": [
        "Startups"
      ],
      "state": "published",
      "title": "Post-Hair of the Dog"
    },
    "-NRRWBcb00SKDnMZtPhS": {
      "body": "To literally _everyone's_ surprise, this blog went haywire the moment classes actually started to matter. It probably didn't help that my DIY text editor failed to save a 700-word post several times in a row... nevertheless to all my incredibly loyal readers I'm happy to announce my (temporary) return. \n<br></br>\n<br></br>\n\nWhy? Well, with less than 12 hours left until I'm forced to fly back to *the place that shall not be named*, I find myself painfully unwilling to sleep and even less disposed to do anything productive with my time. My unpacked suitcase is giving my dirty looks, but I am a servant to my 0-3 readers. What follows might be interesting (or not).\n<br></br>\n<br></br>\n\n$ My abandoned notes from February (yuck)\n- Bitmasks as a low-level solution for one-of-many tag matching. __(??) Your guess is as good as mine.__\n- My newfound poker addiction and the self-rationalization required to convince myself I wasn't just a degenerate gambler. __I've had the only winning sessions of my life since then. We're really becoming some 0.05/0.10 crushers 🤑.__\n- Engineering blogs are an incredible standard. __Cringe but I stand by it.__\n- I love abstractions. __I must have been drunk.__\n<br></br>\n<br></br>\n\n\nAbout 250 words into this post, I'm already getting bored and running out of things to write about. Let's stick to this bullet point thing and call it a _concept_.\n<br></br>\n<br></br>\n\n$ Some numerical highlights since:\n- __1+,__ average number of Zest Plant Powered Natural Energy Drink that I've made a habit of _procuring_ from CharMar on a daily basis.\n- __~2550,__ number of lines it took to build out a v0.1 beta for Palet (code not snow).\n- __90%,__ percentage of that code that is complete dogshit\n- __10%,__ percentage of that code copied from various obscure GitHub repos / GPT-3\n- __8__, number of times I dragged myself out of bed to workout\n- __6 +/- 2__, number of times I did 10 pullups and 20 pushups and called it a workout\n- __40%__, average violence meter\n<br></br>\n<br></br>\n\n$ New vices/coping mechanisms:\n- 🍄\n- Lana Del Rey\n<br></br>\n<br></br>\n\n\nThis post is losing organization (and readers!) fast so let's call this next section:\n<br></br>\n<br></br>\n\n$ Other thoughts we're pulling out the garbage can:\n- I'm come to the realization that I probably won't be able to find a good job for this upcoming fall, so I'm really looking forward to coming back to campus (😐). On the brightside, I've basically finished all my core classes and can finally take some fkoff electives. Well, not really but you can rest assured that I will most definitely not be taking some shit called \"Numerical Linear Algebra\" any sooner than I need to.\n- Martin Shkreli and George Hotz are somehow the exact same person. WOW I had a really outta pocket following statement that I had to delete on a second scan.\n- Don Draper really needs to be talked about in the $\\sigma$-GOAT conversation along with Patrick Bateman, Jordan Belfort, and Christian Bale's Batman\n<br></br>\n<br></br>\n\nYikes. To be honest, I really thought I might have a few more interesting things to talk about after such a long vacation, but it's surprisingly hard to dig up every little thought from the last couple month (shocker!).\n<br></br>\n<br></br>\n\nGetting into YC is my wet dream. Zoox please respond to my job app. \n<br></br>\n<br></br>\n\nCiao.\n\n$s$https://open.spotify.com/track/1wTopxO5eQBpxrBXPSbsUn?si=125b8cfc43484a83$d\n\n$s$https://open.spotify.com/track/13b4mk5KeJxL0GllHLvtXQ?si=441a7cdc595249cd$d\n\n$s$https://open.spotify.com/track/439X8jGytErRiPnaoUJHju?si=26f9fb84b8e441e9$d\n\n$s$https://open.spotify.com/track/5mKiwDDrwG22qKKVL6JZqF?si=c3c5aed211ba419e$d\n\n$s$https://open.spotify.com/track/27dLE8w9LrbjMNU88kUv0G?si=776f98968c8d404a$d\n\n\n$s$https://open.spotify.com/playlist/6TG1eDiPli5fEMeAFjsFeZ?si=c3d91a55e59840ab$d\n\n<br></br>\n<br></br>\n<br></br>\n<br></br>\n",
      "createdDate": "2023-03-26T06:04:37.300Z",
      "labels": [
        "Meta"
      ],
      "state": "published",
      "title": "Where have we been?"
    },
    "-NRaZM2LaowSDO0O8T6B": {
      "body": "__Disclosure: I forget to capitalize my course titles consistently and overuse the word rigor.__\n\n<br></br>\n<br></br>\n\n$ Bruh\n\nAMS 430 a.k.a Intro Stat is a challenging, highly-rigorous introduction to mathematical statistics at the upper undergrad/lower grad level. Topics covered in the course are fundamental to statistical theory and are supposed to provide students with a bedrock foundation for further study in any field that requires statistics, which happens to be just about everything.\n<br></br>\n<br></br>\n\nThat's the gist of the course description in SIS, and in reality Stat fails to accomplish much of that while being one of the most frustrating courses I've taken in my short time at Hopkins __(Second pass: Yikes we're almost half way)__. Its issues are also endemic to the AMS major as a whole—a messy mix of hand-wavy baloney and pick-and-choose rigor. \n<br></br>\n<br></br> \n\nHistorically, I've mentally mapped the courses I take into 2 main categories—solvers and provers. Solvers are built around developing and understanding an arsenal of mathematical / logical techniques that allow you to attack a vast variety of problems. Included are courses I've taken like Intro Prob (AMS 420), Orgo 1,  and Algos (CS 433). At its core, these courses tend to sacrifice marginal rigor to build intuition and application (not to say that they aren't necessarily rigorous nor challenging), and in general I've found these to be pretty easy to deal with as a student. See the techniques, practice them until they become patterns, and come exam day pretty much any reasonable question falls neatly into some predefined mental groove. \n<br></br>\n<br></br> \n\nOpposite these are provers, which contrary to what you might think also includes many of the writing / humanities courses I've taken, e.g. Expository Writing, along with some classics like Real Analysis I (AMS 4XX), Honors Linear Algebra (Math 217, UMich) and Automata Theory (CS 3XX). These courses are deeply theoretical and rooted in rigor and logical artistry, and though Expos might seem out of place, from a different lens it actually shares many characteristics with the type of creative and logical thinking required from any serious proof-based course. These courses can be difficult to study and understand, and it may often take hours and hours of deep thinking for, say, the \"squeezing\" argument in the proof of FTC part 1 to actually click. However, once that light miraculously lights up you've built that mental model so concretely that you can pull it from thin air in whatever problem set / oral exam / written final comes your way.\n<br></br>\n<br></br> \n\nI recognize that I'm painting in broad strokes and some courses definitely blend those lines, but the main takeaway is that identity and cohesion in a course, or at least my ability to build a mental model around what a course is actually accomplishing, has been central to my learning experience __(Second pass: cringe)__.\n<br></br>\n<br></br> \n\nReturning to 430, I'm not here to complain because I find the class to be too difficult (I've definitely been through harder) or the professor annoying (Avanti Athreya is charismatic and a great lecturer). In general, I just take grief with the fact that the course is definitively neither a prover, nor a solver, nor an apple, square, box, zebra, etc.\n<br></br>\n<br></br> \n\nIn lecture, focus is frustratingly presented to rote theoretical derivations that, while definitely descriptive (often routing my attention to online chess), is shockingly far from granting real intuition __(Second pass: the most important part of math IMO)__ or real mathematical rigor. Perhaps this is just a vice of the course because it presumes no analysis background in a field of study where analysis is *literally* everything, and perhaps statistics is just difficult to build intuition around (although my guess is that it really isn't), but that's the way things are.\n<br></br>\n<br></br> \n\nIn contrast, the homeworks somehow expect students to come up with creative solutions to fundamental proofs when no attention is given to canonical techniques or providing crucial context to whatever formula / lower bound / asymptotic behavior is dropped in our face. Then, as if to just put students through a spin, the course tosses in random brute computation and algebra, especially on exams, not to mention many of the gross, gross, gross calculations that have pushed my LaTeX to the very edge. \n<br></br>\n<br></br> \n\nI know this post might just seem like a salty rant and I'm not presenting this very eloquently, but the bottomline is that 430 seems to lack a central thread and tends to dangle students along with the misconception that a low exam average equates to rigor. This trend is bigger than itself, and it often seems like AMS feels like such an easy double major because it lacks the core austerity of a proper, single-major treatment of applied mathematics. \n<br></br>\n<br></br> \n\nWith my perception of what the lower-level experience looks like in Calc 1-3, Linear Algebra, and Diff Eq. at Hopkins, it really just seems like a lot of people aren't tossed into the muck with actual preparation (not to be pretentious, I took 3/4 of those at a community college). In comparison, many peer institutions put even the *applied* mathematics students though core *pure* math courses like analysis and modern algebra, or at the very least a theoretical undertaking of linear algebra (focus on vector spaces, not matrices).\n<br></br>\n<br></br> \n\nTL;DR: I grow 10 grey hairs everytime Prof. Athreya says \"certain regularity conditions\" to gloss over very _uncertain_ analytical ideas in continuity and Riemann integration.\n<br></br>\n<br></br> \n\n__Disclosure part 2: The class isn't that bad and actually is a pretty good intro to upper-level statistics. Prof. Athreya really makes the most of the blank slates she's given IMO and I for one couldn't do any better lol.__\n\n<br></br>\n<br></br>\n\n\n$ The Theory of Everything\n\nThis is the name of a seminar course I'm taking this semester that meets once a week for 3-hours (😐). I just started working on my final presentation which I'm hoping will be a critical look at some famous results in logic / computability theory, e.g. Gödel's Incompleteness Theorems, Tarski's Undefinability Theorem, The Halting Problem, _Eitenschungproblem_, and how they relate to the existence of a supposed ToE.\n<br></br>\n<br></br>\n\nShockingly, I'm actually... not dreading the next few weeks? I finally get an excuse to act like a pseudo-intellectual (a la Cole Sprouse) and read / present a bunch of abstract nonsense. I'm also looking forward to actually making use of some cool features in my Obsidian Vault (__Second Pass: application, not minecraft__).\n<br></br>\n<br></br>\n\nElon I'd totally work at Twitter man.\n\n$s$https://open.spotify.com/track/45zDZbCnp5jwheOENcFG2p?si=e8150c9f7c674f85$d\n\n$s$https://open.spotify.com/track/3FTtAV8d9qHyMjlvAGOPdu?si=aa080e7d27ea406d$d\n\n\n$s$https://open.spotify.com/episode/3bzUO7Onq8SuEt46DnUw8u?si=afdd4a9db22948e8$d\n\n$s$https://open.spotify.com/episode/3a3DMoL76FBVSLj7VUxcrE?si=c63d3a8404144dcc$d\n\n<br></br>\n<br></br>\n<br></br>\n<br></br>\n",
      "createdDate": "2023-03-28T04:54:38.358Z",
      "labels": [
        "Math",
        "School"
      ],
      "state": "published",
      "title": "The Problem with 430"
    },
    "-NRdtNtzgBxVKNVhjHBS": {
      "body": "**Disclosure: Wrote this in class. Hardly proof-read.**\n\n$spin/25/We're on a roll!\n\n$ More pseudo-intellectual spew\n\nMy favorite substack writer, Byrne Hobart, puts out a really slick weekly newsletter on \"inflections in finance tech\" (it's easy to see why I'm a fan). \n<br></br>\n<br></br>\n\nAside from the wise commentary and nonobvious analogies, I also enjoy _The Diff_ for it's \"Long Reads\" section, a collection of 5-6 articles, stories, and other substack/medium blog posts that Hobart recommends. His rationale: the niche online blog community is like a vast river of information, and the writer's job is to as as a targeted tributary to their readers. In exchange for thought-provoking reads and low-effort content discovery, these readers (oftentimes fellow writers) themselves branch-off carrying the sediment of personal experience and unique perspective, quickly creating a massive web of recursive, nuanced discourse.\n<br></br>\n<br></br>\n\nIt's a cool thought to think about and even cooler to see in practice, especially when it comes to massively public events, e.g. the recent SVB collapse, which often results in a massive cascade of online analysis, criticism, and knowledge-share that writers like Byrne Hobart effectively contribute to and then pass along to, say, 50K+ weekly readers __(Second pass: what run-on sentence)__. Substack even seems to recognize this effect and grants blog-authors a dedicated section to recommend others. At a low-level, it's allowed me to discover the writing of some really exceptional individuals like Elad Gil, early investor at Airbnb, Coinbase, Pinterest, etc., or even mixed-media like _The Logan Bartlett Show_. **(Second pass: My hyperlinks don't work anymore 😐)**\n<br></br>\n<br></br>\n\nI initially wanted this post to be my own little tributary, but in the process of writing I started to question my own motivations for writing in the first place. I eventually came to the conclusion that I'm doing this all because it's a space that's wholly my own and where I get to wholly express my thoughts and personality. Did I really want to share a few articles out of the kindness of my heart? Probably not, but I maybe deep down I *did* just want my 0-3 readers to take a second to think \"oh that's interesting\" and build some abstract mental association between those ideas and myself as a person. \n<br></br>\n<br></br>\n\nI might be making myself look an egotist here, but I find it hard to believe that other people my age don't do a lot of things for very similar reasons. Did you really love LeBron's statline so much that you *had* to put it on your story? Did that aesthetic instagram post *really* speak to you that much? What are you trying to say by putting that vsco in your bio? I'm really not trying to be all profound and \"haha social media iphone screenager\" right now and it's definitely not a new idea that social media is this massively telegraphed mess of virtue signaling and show-offiness, but I guess I've just started to question whether that behavior has started to bleed into other parts of my life. __(Second pass: another run-on)__\n<br></br>\n<br></br>\n\nI must admit that 90% of the reason I spent several hours fully customizing my iTerm2 with flashy colors and slick fonts is because it looks cool when I boot it up in class with someone looking over my shoulder. But, I do think that primal urge to be just a little different, to _feel_ just a little different extends beyond myself. In fact, it's one of the things we want Palet to take advantage of.\n<br></br>\n<br></br>\n\n__TL;DR: Straight from the notes of my last _trip_: \"shroom make me think more human. smart people understand bare man in wild.\"__\n\n<br></br>\n<br></br>\n\nWith all that being said...\n\n$ My unsolicited recommendations\n\n$l$Defensability & Competition$https://blog.eladgil.com/p/defensibility-and-competition.\n\nElad Gil (of 2 paragraph's ago fame) on competitive dynamics in startups and a few tidbits on the future of OpenAI-as-a-service companies.\n\n<br></br>\n<br></br>\n\n$l$Jane Street$https://www.thediff.co/archive/jane-street/\n\nEven if you think you understand the general business model of prop trading/HFT firms like Jane Street, Byrne Hobart (!!!) shows you that you don't. **Disclosure: understanding a *Diff* article might require a few trips to investopedia.com (not in a pretentious way!).**\n\n<br></br>\n<br></br>\n\n$l$Ramp so smart big brain$https://engineering.ramp.com/merge-queues\n\nA masterclass on how even the simplest things (literally pull requests) can be engineered to hell. **Disclosure: I would love to work for this company.**\n\n<br></br>\n<br></br>\n\n$l$I love George Hotz$https://www.youtube.com/watch?v=AerjS7PTNYs\n\nNothing else to say. 3\n\n<br></br>\n<br></br>\n\n$ Some YC W23 startups I love\n\n$l$Glass Health$https://glass.health/\n\nPeople keep asking me as a CS major if I'm worried that I'll be out of a job because of ChatGPT. I point them at this company and say that if LLMs ever get good enough to write complex, production-level code for a modern application, then doctors will have already been out of a job for 10 years. \"Notion for Doctors\" is also just such a slick tagline and one of the co-founders went to UCSF or something—incoming 20M+ Series A.\n\n<br></br>\n<br></br>\n\n$l$traceloop$https://www.traceloop.com/\n\nLLMs might not be able to write serious application-level code yet, but writing tests is far lower risk and equally as important at a marginally lower level of complexity. I watched this video once from the former Head of Engineering at Uber on how they took 3 years to develop a proprietary integration and testing system for their 500+ microservices. Imagine a fully-personalized, out-of-box solution instead.\n\n<br></br>\n<br></br>\n\n$l$BERILIUM$https://berilium.com/\n\n I had to choose at least one non-AI company. Democratizing investments in private markets has been a big trend recently and there's honestly nothing special about this one in particular. Lots of people want to be the next Wealthfront and I'm betting that at least one of these eventually does. On the otherhand, we could totally see some macroeconomic reversal that totally leaves illiquid asset classes in the dirt. I'm not smart enough to know if this is a good pick or not but I think it's cool.\n\n<br></br>\n<br></br>\n\n$l$Parlan$https://www.parlan.io/\n\nThis is what I thought Airtable was supposed to be.\n\n<br></br>\n<br></br>\n\n$l$aiFlow$https://www.aiflow.solutions/\n\nGonna need to button up the marketing for Apollo to take this seriously, but I could actually see some serious usage from search funders/boutique funds.\n\n<br></br>\n<br></br>\n\n$ Now the ones I hate...\n\n$l$clearspace$https://www.getclearspace.com/\n\nThis is where dopamine-detox-tok has brought us. It feels like someone will just make a free version of this app (fuck is a  $15 subscription!), hell, I might just make it myself. It's really hard to tell what value this app actually adds aside from those played out \"breathe in! breathe out!\" animations.\n\n<br></br>\n<br></br>\n\n$l$Sanvivo$https://www.sanvivo.eu/\n\nI really hope I'm not shitting on some elderly mfs who can't drive to the store to pick up their pills, but I really thought we were done with \"Uber for X.\"\n\n<br></br>\n<br></br>\n\n$l$Forfeit$https://www.forfeit.app/\n\n🤣🤣🤣🤣\n\n<br></br>\n<br></br>\n\n**TL;DR Feel like Eric Andre rn.**\n\n<br></br>\n<br></br>\n<br></br>\n<br></br>\n",
      "createdDate": "2023-03-28T20:25:22.946Z",
      "labels": [
        "Meta",
        "Startups"
      ],
      "state": "published",
      "title": "Tributaries"
    },
    "-NSmJaW6DZM3E_RMJTGZ": {
      "body": "$ YC Application #1\n\nI submitted my first (of probably many) YC applications this past week, and for many reasons I have relatively low confidence that we will make it through. However, this definitely doesn't mean I have 0 hope—after all, who would I be as a founder if I had that little faith in our idea. My level of confidence can probably be summarized as such: P_{Acceptance}(Palet) = 0, but f_{Acceptance}(Palet) > 0. __(Second Pass: This isn't a bug, I'm just too lazy to implement latex in this editor)__\n<br></br>\n<br></br>\n\n$ A few reasons I'd say no...\n\n- Palet is consumer no matter how much we believe there's enterprise potential at a certain scale. Our target users are people my age and not much older, so we're competing for digital attention that Instagram, TikTok, Snapchat, etc. are already extremely efficient at capturing. A quick look at the 263 startups YC funded in W23 suggests that only ~10 could be categorized as consumer, and only maybe 2 that are social.\n<br></br>\n<br></br>\n- Music/entertainment is a dangerous game to play, and the incumbents we're hoping to tackle are deeply entrenched (the opening line to our pitch literally says we want to rival SoundCloud and Spotify). Silicon valley investors have been historically adverse to music plays for a good reason—many have already tried and failed. It's sooo easy to take a simple look at what we're building and notice that it bears striking similarities to what Michael Seibel would probably call a \"tarpit idea\".\n<br></br>\n<br></br>\n- Although having a technical founder (myself) separates us from a very large pack of nonserious startups and business majors, I'm not necessarily credentialed enough to immediately convince a pack of VCs that I can build anything that needs to be built. On the other hand, woe are the days where Stanford CS + 2014 Facebook are the brand you might need to inspire a minimum level of confidence, partially due to the thousands of DX startups YC has funded over the last decade. Still, a heads up comparison between myself and the dozens of Stanford/Berkeley kids also applying is brutal.\n<br></br>\n<br></br>\n- Big portions of our success depend on certain *intangible* growth factors. Although it might be true that we're just the team to execute upon them **(Second Pass: My favorite question to ask is \"What would Zuck do?\")** **(Third Pass: In an unironic, non-idolizing way)**, it's not really the kind of thing you can throw in a pitch deck and explain effectively to a couple 40-something HBS alum.\n<br></br>\n<br></br>\n\n$ The 2 reasons they could say yes...\n- Our founding team makes sense. Between our collective experience, story, and skills, I'm really binging on the hope that a couple sympathetic angels see potential in a few sharp and ambitious kids.  (**Second Pass: Not self-fellating, just the way we wanna come across**). We're down _as fuck_ for the cause and the capability of our team is certainly not going to be a limiting factor.\n<br></br>\n<br></br>\n- We're trying to address a genuine problem and the market opportunity is there. We threw some really zealous bottom-up TAM figures in our application, but it's undeniable that a significant level of disruption could be a really asymmetric opportunity.\n<br></br>\n<br></br>\n\nWhatever comes from this application, I'm sure I'll look back at the minute-long founder video we recorded in Alex's apartment with lots of nostalgia in a few years.\n<br></br>\n<br></br>\n\n\n$ Notes:\n- I recently started reading through Hacker News (YC's in-house forum) *a lot*. It kind of scares me how much I resonate with the stuff people post on there in all its infinite varieties, and how effectively a website literally called *Hacker News* so perfectly encapsulates all my interests. It's also insane how much brogrammers / nerds love blogs and I guess I truly am no exception. There's no better way I'd rather spend my statistics section than to read degenerate conversations about RabbitMQ,, category theory, and big-Oh optimal notetaking. Startup engineering blog posts about niche horizontal scaling scenarios are paradise.  \n<br></br>\n- Last summer I pirated a copy of the famous *Designing Data-Intensive Applications* and found myself utterly lost. A few months of learning-through-building and a computer systems course later, I'm proud that I've built a somewhat sufficient background to read what is essentially a graduate-level text on backend software engineering. I've spent a majority of this 4-hour bus ride to NYC blasting Roy Blair and skimming through the chapter on databases, and I'm only slightly embarassed to admit that it was a good time. In the words of Jordan Jenkins, \"tangible progress do be sick as fuck\".\n<br></br>\n<br></br>\n- I met Fred Again in the bathroom line at Nublu the day before he did his Tiny Desk Concert. I can not believe this sentence is 100% true.\n<br></br>\n<br></br>\n\n$s$https://open.spotify.com/track/6B2uIq7JV0UfleZOHAulKD?si=9344ecf32fc34ecb$d\n\n$s$https://open.spotify.com/track/1I91L9XVh9Epfmsyb9iI2D?si=a0d3921699d341d4$d\n\n$s$https://open.spotify.com/track/5u1vw4nmCG8iuerg6ppBf0?si=ef3b123171264da0$d\n\n<br></br>\n<br></br>\n<br></br>\n<br></br>\n",
      "createdDate": "2023-04-11T21:56:56.007Z",
      "labels": [
        "Career",
        "Startups"
      ],
      "state": "published",
      "title": "Bus Ride to NYC"
    },
    "-NTqsGRxCMUmLQGiYIhR": {
      "body": "This post is really fking long, so in this edition...\n- Medium length spew about new long-term career interests\n- Short spew about fall course registration\n- Medium length spew about language models\n- Notes on EDM, Louis Vuitton, music, etc.\n\n<br></br>\n<br></br>\n\nAt some point I'll wisen up and implement some sectioning / quick links feature but for now this is all we got!\n\n$br\n\n<br></br>\n<br></br>\n\nA couple of weeks ago, I started filling out a number of online applications for fall co-ops (unfortunately, sparse). Somewhere near the end of these, companies often like to ask for links to some subset of {GitHub, LinkedIn, etc.}, and at some point while autopiloting my way through some Tesla application (pun intended) I realized that I finally had an 8-character string that would fit nicely in the previously untouched \"personal website\" section. It only took me about 2 weeks to realize that there's certain content on here that I probably _don't_ want a potential employer reading...  \n<br></br>\n\nA long time ago, I started thinking that some brute verification/auth system for what effectively functions as my online diary might be smart, but then I came to my senses and realized that A) I'm far too lazy to do that and B) I couldn't do such a disservice as inconveniencing my loyal 1-3 readers. Conveniently, Tesla's one of the few companies that I've actually gotten in touch with recruiters for, so if you're reading this {insert early career technical recruiter} then I hope you know that __I'm a great team player and my biggest weakness is that I work too hard!__\n<br></br>\n<br></br>\n\n\n$ Career Bullshit (I'm very employable)\n\nIf you were to ask me what I thought I'd end up doing after college just a couple months ago, I'd probably tell you that I was really interested in consulting (original, I know). Why? Probably some mix of being a prestige whore, unfounded egotism, \"I've always loved problem solving\", and dread for a potential career in software simply implementing the delusions of a balding McKinsey/GSB alum. This is genuinely where I saw myself ending up and a path I started to walk last summer (although I've definitely made some strange decisions since), but when I sat down earlier this year and finally started putting a plan in action, I found myself sprinting into an unmoving mental block.\n<br></br>\n<br></br>\n\nFinally coming to realize that I had no real interest in consulting was pretty easy, and I definitely feel some unfounded moral superiority towards myself after the fact. However, I do have some genuine motivations for now wanting to pursue other goals. Without making this post excessively long, it generally comes down to realizing I'm simply not a _people_ person, and in some pretentious way I'd rather push myself to be intellectually validated not just by others but also from within. It'd also be easy to say that my lack of interest in the actual day-to-day should've been a larger red flag earlier on, but if we're being real who goes into consulting as an act of passion?\n<br></br>\n<br></br>\n\nThe _more_ difficult question I've had to answer within myself is what I now want to do in as soon as 2 years. As sure as I am that my answer will keep changing, as I told Will in a later-night conversation a couple days ago, I pretty much only see myself enjoying graduate school and / or some balls-to-the-wall startup __(Second Pass: also pretentious)__. As much as I'd love to bet on Palet achieving enough success in the next 6 months to justify dropping out, it's probably a bit smarter on a risk-adjusted basis to plan for a horizon past that.\n<br></br>\n<br></br>\n\nWhat does this really change? In the short term, not much. In fact, I realize that a majority of the things I've pursued or spent time learning / observing over the past year have been useful at validating and orienting myself in this direction __(Second Pass: except for the hundreds of hours I've spent laying in bed on yt)__. However, this _does_ mean that I'm a little less enthusiastic about locking in a W2 job for this upcoming fall as opposed to spending time on campus taking much cooler classes (see below) and getting involved in research.\n<br></br>\n<br></br>\n\n__TL;DR: I have a low tolerance for stress and this new direction lets me put it off for a couple more months.__\n\n<br></br>\n<br></br>\n\n\n$ Course Registration\n\nI ended up writing an 80-line python script this past week to try squeezing myself into a few courses that are disproportionately popular relative to class size. It's pretty barebones and doesn't even utilize anything particularly clever / novel, but it still manages to register at latencies many multiples smaller than even the fastest human reflex. After a few days of nervous testing, the results ended up being pretty incredible. While I'd originally sought out to shotgun 5 courses and hope to get into a just a couple, here's what ended up happening:\n<br></br>\n<br></br>\n\n```\n> python reg.py 2023/04/19/07/00 tbai4@jh.edu <password>\nWaiting...\nWaiting...\nWaiting...\nExecuted at 2023-04-19 07:00:00.0023757\nEN.601.461, Computer Vision                   Enrolled!\nEN.601.465, Natural Language Processing       Enrolled!\nEN.601.468, Machine Translation               Enrolled!\nEN.601.475, Machine Learning                  Enrolled!\nEN.601.482, Machine Learning: Deep Learning   Enrolled!\n```\n<br></br>\n<br></br>\n\nNeedless to say I'm pretty happy, but I'm now faced with the difficult decision of choosing some subset of all these. Although NLP with Eisner seems like a no brainer, I'm unsure what mix of other courses are most relevant to my potential interests / would be most educational / wouldn't be too much work on top of potentially still working with Palet and wanting to do 15-20 hours of research each week. Part of me even wanted to register for EN.553.740 Machine Learning I, a PhD-level treatment of ML from the math department, but I'm clearly a bit too high maintenance __(Second Pass: the professor also heavily insinuated that I'd bomb the class. he doesn't realize I'm a dawg at analysis 🥶)__.\n<br></br>\n<br></br>\n\n__TL;DR: My life is so hard.__ \n\n<br></br>\n<br></br>\n\n$ LM Commentary\n\n$\"AutoGPT has been all the rage this past week and things are actually starting to get pretty spooky... the speed at which we're able to iterate and build upon existing state-of-the-art techniques and models is mind blowing. We're definitely reaching an inflection point where the rate at which we're able to learn and build, learn and build, learn and build seems to compound infinitely. It's becoming increasingly hard (at an exponential rate) to reject the notion that AGI is right around the corner. (Second Pass: Character AI biiiiig funding round 😵‍💫).\n\n<br></br>\n\n\nA couple days after I wrote the above, I skipped out on my Science and Film class to attend a talk by Karthik Narasimhan, former OpenAI research scientist and assistant professor at Princeton, on language-enabled autonomous agents. Narasimhan's been working on language models for years before they recently got the hype and even has his name on the _original_ GPT1 paper. \n<br></br>\n<br></br>\n\nOn top of being a great high-level presentation on various cutting-edge research applications of language models, what really stood out was the way various faculty would periodically interject with insightful questions, a highly underrated skill I'd previously never really thought about. Jason Eisner, a pretty big name at the CLSP, was especially poignant throughout and it's easy to see how he's achieved all the research success he has to this date.\n<br></br>\n<br></br>\n\nI know it might seem like I'm just hopping on the bandwagon but I can actually see myself really enjoying research along the lines of Narasimhan—some blend of theory, methodology, and application that touches on many of my favorite areas of probability / statistics / analysis / programming. After digging deep into the publications of a couple faculty at Hopkins working in this area, I was also surprised at how accessible the work seemed to be at from a mathematical level and how practically intriguing it was to me personally. It's also a huge bonus that pursuing graduate work in this field and being good at it would probably guarantee someone decades of easy funding, lucrative industry opportunities, and the feel-goodiness of bringing the world one step closer to an AI-dystopian future. \n<br></br>\n<br></br>\n\nThere's definitely a big part of myself that wishes I had discovered this interest earlier, or even worked a bit harder in high school such that I'd have the research and learning capacities available at a Stanford / Berkeley / MIT / CMU, but if there's one silver lining it's that Hopkins is decently (arguably highly) above mediocre in NLP/ML stuff (moreso NLP). This is definitely a \"can't change the past\" moment and I'm excited (+ a little scared) to see if I can actually execute on the lofty, egotistic perception I have of myself.\n<br></br>\n<br></br>\n\n__TL;DR: I hope I'm cut out for what I seek.__\n\n<br></br>\n<br></br>\n\n$ Closing notes\n\n- The Four Tet / Skrillex / Fred again.. trio has been doing the rounds recently for their eclectic closing set at Coachella. It's pretty surreal how far EDM has come over just the last year and also how much Fred again..'s viral success last summer seems to have contributed to it. Have I ever mentioned that I met him in the bathroom line at Nublu?\n<br></br>\nI definitely wish we'd made quicker progress on Palet to best capitalize on this opportunity, but if anything this has just lit a huge fire under our asses and motivated us to work much faster / harder. Hopefully, big things to come. \n<br></br>\n<br></br>\n- LVMH has just become the first European company to exceed a $500Bn market capitalization. I skimmed through the 10k earlier today and was pretty shocked at the fucking insane monopoloy they seem to have on all high-end consumer goods (I would list a couple of them but there's literally so fking many). Someone on HN recommended a book about the savage methods LVMH used to consolidate European fashion in the mid 2000s and it's definitely an add to the reading list.\n<br></br>\n<br></br>\n- It's a great / strange feeling to come up with a solution to a problem and realize that what you've come up with is already some well-known standard. I thought I was some kind of genius for using \"?origin=X\" query string parameters on paletapp.com to log where we're driving maximum clickthrough (there's probably some future post on here about the logging and analytics stack). \n<br></br>\n<br></br>\n- I popped into the code repo for this site earlier today to add that little grey linebreak in the \"table of content\" section at the start. It kind of struck me as I was adding the 20th \"else-if\" clause that backwards compatibility will become a real pain if I ever rehaul the way this site renders markdown. Whatever.\n<br></br>\n<br></br>\n\nThis is by far the longest post I've ever written, so if you've made it this far (or just scrolled to the very bottom) then don't you have better things to do with your time?\n<br></br>\n<br></br>\n\n再见！\n\n<br></br>\n\n$s$https://open.spotify.com/track/3CblJq8QQQ0bb7vwJu8c3v?si=12c4895a21c54a43$d\n\n$s$https://open.spotify.com/track/3uouaAVXpQR3X8RYkJyitQ?si=ef8510b5de4b4cfa$d\n\n$s$https://open.spotify.com/track/0judK8TqYfvwUohBngUxrE?si=397fc05ec98d4cb1$d\n\n$s$https://open.spotify.com/track/5zuex7tbGFz0gSHlrMvQdu?si=c16a3dd8ae1f4c93$d\n\n$s$https://open.spotify.com/playlist/20ZuYHp4eGeHecE52KE8Sy?si=2c245d2c3435422b$d\n\n\n<br></br>\n<br></br>\n<br></br>\n<br></br>",
      "createdDate": "2023-04-25T05:26:57.638Z",
      "labels": [
        "Career",
        "AI"
      ],
      "state": "published",
      "title": "New Direction"
    },
    "-NiLJljzza7Ff974t3Nx": {
      "body": "We're back!\n\n<br></br>\n<br></br>\n\nFunctional programming (FP)—a paradigm of building computer programs using expressions and functions without _mutating_ state and data.\n\n<br></br>\n<br></br>\n\nDespite my opposition towards taking software engineering or tutorial-type CS courses at Hopkins, I ended up registering for \"Functional Programming in Software Engineering\" as a last-minute replacement to the heftier Computer Vision/Deep Learning. Really, I sought an easy option that would let me invest time in Natural Language Processing and Machine Learning, but that also wouldn't be a total waste of time. After all, finally demystifying type systems, functors, and monads seemed more than worth it for an easy A and 3 credits.\n\n<br></br>\n<br></br>\n\nUp to this point, my entire exposure to FP was through a brief survey of category and type theory in summer 2022, as well as having once read an article that Jane Street made the (relatively) esoteric _OCaml(1)_ their de-facto programming language (nerds). \n\n<br></br>\n<br></br>\n\nDespite wrestling with the many idiosyncracies of OCaml for the better part of a month, I think I've finally started to _grok(2)_ how to effectively implement non-trivial ideas. A few thoughts:\n\n<br></br>\n<br></br>\n\n- FP really is a pleasant experience when you get your brain out of thinking in dumb, low-level imperative processes. _State_ is ephemeral, and transforming it without explicit pre- and post-conditions is just dangerous mental overhead.\n\n<br></br>\n<br></br>\n\n- FP lets you write incrementally correct and easily testable code. The benefit of strong and _exhaustive_ pattern matching (which used to be a pain), is being able to clearly categorize your thoughts as a branching set of decisions.\n\n<br></br>\n<br></br>\n\n- Strong type systems can be frustrating when you need to coerce them into making \"easy\" assumptions, but beautiful when they start to teach you about what your code is actually doing (as opposed to what you think it's doing in your head).\n\n<br></br>\n<br></br>\n\n- Though I may never use a purely functional language as my go-to for quick scripts or scrapes, it's an appealing option for code I get the chance to think/reason about well before I ever even open a text editor.<br></br><br></br>The FP experience is like a fun little game where you go around completing callback side quests, and piece together the cumulative experience to attack something large and unwieldy. That's all I have to say.\n\n<br></br>\n<br></br>\n\n(1) https://ocaml.org/\n(2) https://en.wikipedia.org/wiki/Grok\n\n<br></br>\n<br></br>\n\n$s$https://open.spotify.com/track/3XRALl9CNCnjjbCZid2L9w?si=3b7debbb3c414eb4$d\n\n$s$https://open.spotify.com/track/0AQ1twwDGVlIVpcHDkGkXD?si=2a3348b22cf44bfd$d\n\n$s$https://open.spotify.com/track/5oBarQJHCnHLzyezz0XzFx?si=6abc073e232d457a$d\n\n$s$https://open.spotify.com/track/6me7F0aaZjwDo6RJ5MrfBD?si=44a8093472de4909$d\n\n$s$https://open.spotify.com/track/0P2P1AxcRYhiZvoF5MyRmM?si=ad70ec076b00498c$d\n\n<br></br>\n<br></br>\n<br></br>\n<br></br>\n<br></br>\n<br></br>",
      "createdDate": "2023-11-03T17:55:10.981Z",
      "labels": [
        "CS"
      ],
      "state": "published",
      "title": "FP is Pleasant"
    },
    "-Nj6FiiK-iTgjZEchZF7": {
      "body": "I write this in complete despair as I enter my _nth_ consecutive hour of debugging a heinous backpropagation error. I used to consider myself a good (NOTE: not great) programmer—never before have I been so humbled at the hands of a few equations and a text editor.\n\n<bs></bs>\n<bs></bs>\n\nThe assignment: write 2 taggers that takes English sentences and _tags_ words with their part of speech. For example, \"The ads celebrate the achievements of...\" becomes \"The/D ads/N celebrate/V the/D achievements/N of/I...\"\n\n<bs></bs>\n<bs></bs>\n\nSimple enough right? How would you do this? \n\n<bs></bs>\n<bs></bs>\n\nSpoiler: it's actually really fking hard. \n\n<bs></bs>\n<bs></bs>\n\nNLP Homework 6 challenges one to consider taggers based off 2 architectures: _hidden markov models(1)_ and a more modern _biRNN-CRF(2)_. I digress—the point of this post isn't to flex my incredible (RE: nubile) knowledge of statistical models, but rather to complain about the incredibly steep learning curve into writing and training the large class of models enabled by modern tensor frameworks like _PyTorch(3)_.\n\n<bs></bs>\n<bs></bs>\n\nI should really get back to work, so I'll keep it short:\n\n<bs></bs>\n<bs></bs>\n- Automatic differentiation/backpropagation is dark magic.\n\n<bs></bs>\n<bs></bs>\n- Numerical instability sucks.\n\n<bs></bs>\n<bs></bs>\n- Tensor products are impossible riddles that manage to insert themselves on every path from theory to efficient code.\n\n\n<bs></bs>\nI think it will take me about 3 more months of running headfirst into many walls before everything magically clicks. In fact, I look forward to the day when I can look back at this post and laugh at myself. Until then, PyTorch is not pleasant and I hate everything.\n\n\n<bs></bs>\n<bs></bs>\n<bs></bs>\n<bs></bs>",
      "createdDate": "2023-11-13T05:58:53.548Z",
      "labels": [
        "CS",
        "Short",
        "School"
      ],
      "state": "published",
      "title": "PyTorch is Not Pleasant"
    },
    "-NjEToSODLQn63r_1cHI": {
      "body": "1 billion dollars to anyone that can explain my own code to me. \n\n<bs></bs>\n<bs></bs>\n\n```python3\n'''\nDO NOT TOUCH\nOne iteration of forward pass for biRNN-CRF parameterized by:\n- M, prefix parameters \n- M', suffix parameters\n- U_A, bigram parameters\n- U_B, unigram parameters\n- theta_A, bigram weights\n- theta_B, unigram weights\n'''\n\n# compute prefix embeddings\npref = torch.zeros(len(sent), self.hd)\nfor i, (w_i, _) in enumerate(sent[1:], start=1):\n    pref[i] = torch.sigmoid(\n    self.prefix_M @ torch.cat((torch.tensor([1]),pref[i-1], self.E[w_i])))        \n# compute suffix embeddings\nsuf = torch.zeros(len(sent), self.hd)\nfor i, (w_i, _) in reversed(list(enumerate(sent[:-1]))):\n    suf[i] = torch.sigmoid(    \n    self.suffix_M @ torch.cat((torch.tensor([1]), self.E[w_i], suf[i+1])))\n\n# A[s, t] = [1; h_(i-2); s; t; h'_i]\nA = torch.zeros(self.k, self.k, 1 + 2*self.hd + 2*self.k)\nA[:, :, 0] = 1\nA[:, :, 1:self.hd+1] = pref[i-2]\nA[:, :, -self.hd:] = suf[i]\nA[inc, :, 1+self.hd+inc] = 1\nA[:, inc, 1+self.hd+self.k+inc] = 1            \n\n# bi_features[s, t] = f_A(s, t, w, i)\nbi_features = torch.sigmoid(A @ self.U_A.t())\n\n# phi_A[s, t] = phi_A(s, t, w, i)\nphi_A = torch.exp(bi_features @ self.theta_A)            \n\n# b[t] = [1; h_(i-1); t; w; h'_i]\nb = torch.zeros(self.k, 1 + 2*self.hd + self.k + self.wd)\nb[:, 0] = 1\nb[:, 1:self.hd+1] = pref[i-1]\nb[:, -self.hd:] = suf[i]\nb[inc, 1+self.hd+inc] = 1\nb[:, -(self.wd+self.hd):-self.hd] = self.E[w_i]            \n\n# uni_features[t] = f_B(t, w, w, i)\nuni_features = torch.sigmoid(b @ self.U_B.t())  \n                      \n# phi_b[t] = phi_B(t, w, w, i)\nphi_b = torch.exp(uni_features @ self.theta_B)\n\n# now we can apply the update rule\na_j = (alpha[-1] @ phi_A) * phi_b\n\n# zero out if tag is known, DP!\nif tag is not None: a_j = a_j * self.eye[tag]            ```\n```\n\n<bs></bs>\n<bs></bs>\n\nIs this actually correct? There's no way to actually tell—but loss goes down so monkey brain says it's fine.\n\n<bs></bs>\n<bs></bs>\n\n```\nINFO:eval:Cross-entropy: 1.1342 nats (= perplexity 3.109)\nINFO:eval:Cross-entropy: 0.6894 nats (= perplexity 1.992)\nINFO:eval:Cross-entropy: 0.6385 nats (= perplexity 1.894)\nINFO:eval:Cross-entropy: 0.6380 nats (= perplexity 1.893)\nINFO:eval:Cross-entropy: 0.6379 nats (= perplexity 1.893)\nINFO:eval:Cross-entropy: 0.6379 nats (= perplexity 1.892)\n2500it [00:44, 55.91it/s]\n```\n\n<bs></bs>\n<bs></bs>\n\nPyTorch is beautiful. I miss the person I was 2 hours ago.\n\n<bs></bs>\n<bs></bs>\n<bs></bs>\n<bs></bs>",
      "createdDate": "2023-11-14T20:17:24.808Z",
      "labels": [
        "CS",
        "Short"
      ],
      "state": "published",
      "title": "PyTorch is Beautiful"
    },
    "-NjOeHhmAZNhjCPp_zex": {
      "body": "This is the kind of title that might really impress people that don't know how easy it _can_ be to write a programming language. In fact, I might even say that I'm the founder and only user of an esoteric markdown language that lets me insert whirly-tirly's in these blogposts—\n\n$spin/40/boom!\n\n(On a side note, the more I try to add functionality to this markdown language the more I realize how terribly it's designed. I also haven't added any features in probably close to 9 months 😵‍💫)\n\n<br></br>\n<br></br>\n\nHowever, I _do_ intend to actually put some serious effort into writing a non-trivial programming language. This is just one of those things—alongside startups, lifting, etc.—where I envision that even undergoing the process might pay off massive dividends for years to come, regardless of the outcome. Plus, I was recently inspired by [___this blog post___](https://blog.vero.site/post/noulith) which describes a lot of the unique considerations and payoffs of designing a personal language.\n\n<br></br>\n<br></br>\n\nI am aware that there's a high probability I look back in 6 months and laugh that I ever thought I could do this, so rather than setting some grandiose, idealistic goal, here are the steps I plan to follow:\n\n<br></br>\n<br></br>\n\n1. My final project for [___FPSE___](https://www.tjbai.me/-NiLJljzza7Ff974t3Nx) is a simple Python-to-C transpiler written in OCaml. I'm dreadfully behind schedule for what's due in just about a month, but this is a solid first step if I have to complete it for a grade anyways. Despite having modules canonically named _lex_, _parse_, _ast_, etc., this project is mostly off-the-dome, partly because I'm worried there isn't enough time to actually write a tree-walk interpreter with code generation the _right way_. \n<br></br>\n<br></br>\n2. Throughout next spring, and probably kicking off over winter break, I want to work through Chapter 2 of [___Crafting Interpreters___](https://craftinginterpreters.com/). I hope to make personal design choices throughout as much as I can, but where necessary I'll stick to the text. Hopefully, after 4-6 months I'll find myself with a concrete, well-designed, and extensible core.\n<br></br>\n<br></br>\n3. Now I can do whatever I want! I have a couple ideas for what I want this language to look like in the end, but I'm sure that vision will evolve dramatically in following months. At the end of the day, I want something I can continue tinkering with for not just months but _years_ to come—if I find some way for this language to provide actual utility in my life then that's just the cherry on top. A solid litmus test might be if I can complete the Blind 75 and/or Advent of Code entirely within it. \n\n<br></br>\n<br></br>\n\nThis is all pretty ambitious and it's really easy to write these steps as I'm currently procrastinating other work I have to do. However, I _do_ need to something to look forward to/motivate me in the spring (historically, a dead time between recruiting and work). \n\n<br></br>\n<br></br>\n\nI also just think `.tj` would be very cool as a file extension.\n\n<br></br>\n<br></br>\n\n$s$https://open.spotify.com/track/5xo8RrjJ9CVNrtRg2S3B1R?si=cf2a3592bbb64c4d$d\n\n$s$https://open.spotify.com/track/1fDFHXcykq4iw8Gg7s5hG9?si=bb8d5644a3c94664$d\n\n$s$https://open.spotify.com/track/5cGZN0P1QnSfhCFBCHtp2N?si=9a10cc370c6942ea$d\n\n<br></br>\n<br></br>\n<br></br>\n<br></br>\n",
      "createdDate": "2023-11-16T19:43:44.505Z",
      "labels": [
        "PL",
        "CS"
      ],
      "state": "published",
      "title": "Writing a Programming Language"
    },
    "-NjupLQSGd0HUbr4yoYg": {
      "body": "In this edition:\n- Long-ish spew about machine learning\n- Short spew about TJL (my language!)\n- Short spew about this shitty blog\n\n$br\n\n<br></br>\n<br></br>\n\n$ Estimators\n\nThere's a really appealing set of canonical ideas/generalizations throughout statistics, machine learning, and related fields that are surprisingly easy to reason about. It's actually pretty incredible that such _easy_ concepts can be so powerful. Easy not necessarily because of simple underpinnings, but because of their _innateness_ (_READ: intuitive, fundamental_).\n\n<br></br>\n<br></br>\n\nI recognize that saying this is really an abuse of academic pedigree, but I'm sort of riding a high right now as I'm ripping through a machine learning pset (assigned over thanksgiving break...). Yet, the fact that I've obtained this hubris after such minimal rigorous study really gives me hope for the _magic_ things that will be similarly unveiled in time to come.\n\n<br></br>\n<br></br>\n\nOne such idea is that of a maximum likelihood estimator, as covered in most introductory statistics classes. Even without a shred of mathematical formalism, the idea is simple—you have some idea for how the world works and you fit this model with the parameters that best explain the data. If I flip 100 coins and I see 53 heads, do I then expect that coin to have around a 53% chance of coming up heads?\n\n<br></br>\n<br></br>\n\nOf course, MLEs are simplistic on their own and often require some kind of _smoothing_ or _regularization_, which themselves open an entire Pandora's box of possibilities. _Aside: understanding regularization as maximum _a posteriori_ estimation is also pretty beautiful._\n\n<br></br>\n<br></br>\n\nThe realm of possibilities you can explore with an idea so simple as maximizing the likelihood is incredible. Throw in some Bayes' Theorem and you have an entire intro course in machine learning, and realistically enough theory to dive head-first into applications. Evidently, I oversimplify, but starting to get thise sense of being able to see through the noise is addicting, and it's _so accessible_.\n\n<br></br>\n<br></br>\n\nI wish I could write a few more paragraphs on all the synaptic connections that seem to have fallen into place recently—words like representation, entropy, latent, etc. and topics like Expectation-Maximization come to mind—but in truth these ideas are way too half-baked. Plus, I run the risk of getting a little overconfident. The bottom line is I just _really_ love [__grokking__](https://en.wikipedia.org/wiki/Grok#:~:text=Grok%20means%20to%20understand%20so,lose%20identity%20in%20group%20experience.) things and this is no exception.\n\n<br></br>\n<br></br>\n\nThe other thought I wanted to develop is _why_ this notion of a _maximum likelihood_ estimator is seemingly so natural, both in derivation and application. Is it this way to the human mind because its origin is just that—a _natural_ cognitive process for how we, as agents, ingest information and build understanding? When I make a prediction, what are the features I extract, the parameters, the connections? How do I learn?\n\n<br></br>\n<br></br>\n\nPart of this duality comes from the simple fact that many great mathematical ideas _are_ inspired by biological ones, just look at the _neural_ in neural networks. On the other hand, can the same be said for the more fundamental ideas of _probability_, _learning_, and _representation_ themselves? Is the [__unreasonable effectiveness of mathematics__](https://en.wikipedia.org/wiki/The_Unreasonable_Effectiveness_of_Mathematics_in_the_Natural_Sciences#:~:text=%22The%20Unreasonable%20Effectiveness%20of%20Mathematics,and%20even%20to%20empirical%20predictions.) simply the side effect of distilling centuries of cognition into notation (_NOTE: I don't know if that makes any sense but I like it_)? I'm sure lots of these questions could be answered with a deeper dive into the [__philosophy of probability__](https://plato.stanford.edu/entries/probability-interpret/) and [__algorithmic information theory__](https://en.wikipedia.org/wiki/Algorithmic_information_theory), but I've had a few of these books on my reading list for months.\n\n<br></br>\n<br></br>\n\nFrighteningly, over time I find myself empathizing more and more with the crowd that asks questions like [__\"Is my toddler just a stochastic parrot?\"__](https://www.newyorker.com/humor/sketchbook/is-my-toddler-a-stochastic-parrot) or that believes true AGI is somehow achievable with modern deep learning architecture. I imagine this is one of those bell curves where I've just started to get enough knowledge to hardly understand the populace (_READ: simpleminded_) opinion. I know that with time comes understanding, so I'm just glad to feel like I've hit a point of inflection in the growth rate of that understanding.\n\n<br></br>\n<br></br>\n\n$ TJLOG #1\n\nI've officially dubbed my [__pet language__](https://www.tjbai.me/-NjOeHhmAZNhjCPp_zex) _TJL_ because:\n\n<br></br>\n<br></br>\n\n1. Uh, it has my name in it?\n2. `tjl` is really easy to type at the prompt.\n3. It reminds me of a language called [__tcl__](https://www.tcl.tk/about/language.html) which I've been meaning to explore.\n\n<br></br>\n<br></br>\n\nRather than make progress on relevant schoolwork, I spent a couple hours brainstorming the general look/feel of TJl, which you can find [__here__](https://github.com/tjbai/tjl/blob/main/design.txt). Obviously, many things up for change but I think I've distilled this language to something that has the ease and utility of Python, but the (occasional) beauty and elegance of OCaml. Incidently, these are the 2 language I've used most in the last 2 months.\n\n<br></br>\n<br></br>\n\n$ A total rewrite?\n\nIt's funny how much can change in just a few months. When I wrote this website under a year ago, I was so confident that I loved the design decisions I made. Although aesthetically, I'm happy, it's dawned on me after a few months of not posting how much of a shit show this site really is.\n\n<br></br>\n<br></br>\n\nThe biggest grievances:\n\n<br></br>\n<br></br>\n\n1. The entire this is a [__SPA__](https://en.wikipedia.org/wiki/Single-page_application) which means links don't work how you expect them to, metadata is fucked, and haven't you heard that, like, SPAs aren't in the vogue anymore? \n2. The editor experience is _garbage_ and I really didn't think at all about what it might look like to extend OR migrate it at a future point, a.k.a now. Backwards compatibility with the text formatting is a serious concern and I don't really know how to fix it without a _total rewrite_. Plus, every once in a while I'll enter an illegal markdown command and the entire thing will get wiped. \n3. I've started to realize the entire site looks better if the font is, like, 10% smaller. Unfortunately, I'm far too lazy to fix it. If I were smarter, I would've just added the font as a magic number somewhere in the Chakra UI `theme.ts`.\n\n<br></br>\n<br></br>\n\nFortunately, part of the rugged charm of this site is that I wrote it all in less than 2 hours, so doing a rewrite doesn't necessarily feel bad or anything. On the other hand, I full recognize that nuking for a new version would require a much bigger commitment than just another 2 hours. What's that saying again, don't fix what isn't broken? Probably wise.\n\n<br></br>\n<br></br>\n\n$s$https://open.spotify.com/track/15DwFznkBJir7AK9PyMyRR?si=56c7d079b5a94c58$d\n\n$s$https://open.spotify.com/track/4FEcEwbE2vsqhxbTPtiNTL?si=317a3cc3d4784bd4$d\n\n$s$https://open.spotify.com/track/4vyG9ZhHT8MKJE5mTICMFC?si=7360fb485b3449ea$d\n\n<br></br>\n<br></br>\n<br></br>\n<br></br>\n\n\n",
      "createdDate": "2023-11-23T06:19:31.335Z",
      "labels": [
        "Math",
        "AI",
        "CS"
      ],
      "state": "published",
      "title": "Thanksgiving Eve"
    },
    "-NkGvO26P71rI4s-2MpV": {
      "body": "$ Entropy\n\nIt feels like every few weeks some new research paper bubbles up tying together generative models and compression with some cute title like [__\"Generative diffusion models are associative memory networks\"__](https://arxiv.org/abs/2309.17290#:~:text=Generative%20diffusion%20models%20are%20a,a%20set%20of%20target%20states.) aka _\"Compression is all you need.\"_ If you're not already familiar with the idea that \"learning is compression\", then it might be worth looking into the [__Hutter prize__](https://en.wikipedia.org/wiki/Hutter_Prize#:~:text=The%20Hutter%20Prize%20is%20a,in%20artificial%20intelligence%20(AI).) or perhaps even a survey of [__algorithmic information theory (AIT)__](https://en.wikipedia.org/wiki/Algorithmic_information_theory), aka Kolmogorov complexity theory.\n\n<br></br>\n<br></br>\n\nIf you need any motivation, AIT provides a foundation for arguments for/against the achievability of [__artificial general intelligence (AGI)__](https://en.wikipedia.org/wiki/Artificial_general_intelligence). In fact, it provides the framework for [__AIXI__](https://en.wikipedia.org/wiki/AIXI), which is a well-defined, yet uncomputable, AGI agent which perfectly maximizes expected reward/payoff from its environment. In a sense, it represents a theoretical pinnacle or upper bound for what we might hope to achieve through _any_ artificial agent.\n\n<br></br>\n<br></br>\n\nI digress—\nThe idea that SoTA foundation models (GPT-4, Dall-E, Whisper, etc.) are nothing more than really complex caches between queries and results is both cool and discouraging. After all, we ___might___ believe that a truly \"intelligent\" agent is capable of much more powerful feats than simply representing large quantities of information in some representation of memory. Notably, the ability to _plan_ is an important topic in classical AI (think minimax, alpha-beta pruning, MCTS) and especially relevant now—[__has OpenAI broken the next-token-prediction glass ceiling with Q*?__](https://www.theguardian.com/business/2023/nov/23/openai-was-working-on-advanced-model-so-powerful-it-alarmed-staff) \n\n<br></br>\n<br></br>\n\nEven if we _do_ believe that the brain is the world's ultimate compression system and that's the primary mechanism through which we learn and act, forcing autoregressive language models to at least _simulate_ reasoning and planning has been beneficial in practice. This is often called [__chain-of-thought prompting__](https://arxiv.org/abs/2201.11903).\n\n\n<br></br>\n<br></br>\n\nI was personally first introduced to the notion that compression, or more broadly _Shannon_ information theory (NOTE: NOT to be confused with AIT, although related), is linked to machine learning relatively recently from a paper called  [__\"Language Modeling is Compression\"__](https://arxiv.org/pdf/2309.10668.pdf), via some nerds at DeepMind. In fact, the very first sentence quotes that the 2 disciplines are \"two sides of the same coin\"—imagine my surprise when I independently read the preface to [__\"Information theory, inference, and learning algorithms\"__](https://www.inference.org.uk/itprnn/book.pdf) and found the same phrase where it was originally written!\n\n<br></br>\n<br></br>\n\nI know I've found the right textbook at the right time when the table of contents is a carbon copy of my browser search history (seriously, an entire chapter on Hopfield networks seems fatalistic). Although reading all 600 pages cover to cover might prove difficult, you (who am I talking to?) can rest assured that I will be _aggressively skimming_, while also investing time where it may prove fruitful. I look forward to it.\n\n<br></br>\n<br></br>\n\nIf you don't know why this section is called entropy, then you need some more information theory in your life. Here's the [__wikipedia article__](https://en.wikipedia.org/wiki/Entropy_(information_theory)) on it and an accessible 3b1b video that uses [__information theory to play better wordle__](https://www.youtube.com/watch?v=v68zYyaEmEA&ab_channel=3Blue1Brown).\n\n<br></br>\n<br></br>\n\n\n$ Ergonomics\n\nI don't know if that previous section was at all readable, so here's a dumber thought I've been playing with recently—when are solutions solving a problem of _ergonomics_ vs. _economics_?\n\n<br></br>\n<br></br>\n\nAllow my abuse of language—\nMany problems are _economic_ in nature. They stem from the challenges associated with maximizing limited resources like time and money. For a startup, tangibly improving your user's economics is a great way to sell. Inducing an environment where everyone's economics are improved through participation is a great way to gain traction. \n\n<br></br>\n<br></br>\n\nBut, I think it's obvious that not all problems (and therefore not all solutions) are _economic_ in nature, or at least not obviously.\n\n<br></br>\n<br></br>\n\nHere's an ergonomic problem I have: every time I have to update something in my shell configuration, I access the relevant dotfile with the command _\"vi ~/.zshrc\"_. I encourage you to type this out on a keyboard and report back with how annoyed it makes you. Way too much pinky action. Resultingly, as I mentioned in my previous post, one _legitimate_ consideration I had when choosing the name for my new programming language was how easy it is to type at the prompt. Try typing _\"tjl\"_. Pretty clean, no? \n\n<br></br>\n<br></br>\n\nIf I sound neurotic, then it's best you know that this is actually considered a [__best practice for CLI design__](https://clig.dev/). Being easy to to type solves an _ergonomic_ problem. It makes my life a bit more comfortable and efficient in the exact same way as a nice office chair. But, \n\n<br></br>\n<br></br>\n\n__I got too lazy to finish this section and didn't want to think about it anymore. I really don't have any grand thoughts—I was just going to walk through some examples and the main punchline is simply asking _how do we model the value of goods that don't have a first-order effect on your economics?_ If I knew the answer then I'd be the greatest investor of all time, so I'm just going to truncate this here.__\n\n\n\n<br></br>\n<br></br>\n<br></br>\n<br></br>\n<br></br>\n<br></br>",
      "createdDate": "2023-11-27T17:57:10.999Z",
      "labels": [
        "AI",
        "Startups"
      ],
      "state": "published",
      "title": "Entropy and Ergonomics"
    },
    "-NlvN-E1ZmY2u8GOtS-x": {
      "body": "$ Language as an API\n\n1. Where structured information exchange exists, APIs exist. Whether writing an HTTP request to OpenAI's chat completions endpoints, flipping a light switch, or hitching a ride from Will to Eddie's, each interaction requires engagement with some pre-defined interface. Rigidity is the only dimension along which these events vary. \n<br></br>\n<br></br>\n2. APIs are frequently structured and inflexible by necessity. Determinism and interpretability are essential to supporting the reliability, availability, and consistency of systems. However, the issues lies in the inverse relationship between structure and compressive power. We are forced to restrict the expressiveness and breadth of a channel when understanding abilities are limited. If I could read 10 books at once, I would.\n<br></br>\n<br></br>\n3. If there's anything I've learned in the last semester, it's that natural language possesses incredible compressive power. Through the intricacies of syntax, semantics, pragmatics, etc. it encodes large amounts of high-dimensional information in a form that is still capable of being developmentally intuited by our brains. Despite many attempts (see: Esperanto), we simply can't brute-force our way to _structured_ (read: synthetic) patterns that come close to recreating the infinite nuances of natural language.\n<br></br>\n<br></br>\n4. The wonder of LLMs, and by extension other families of generative models, isn't just their impressive ability to _generate_, but in signaling the existence of intricate world models and knowledge compression through learned parameters. We've hit a point of inflection where models exhibit the capacity for self-revision, mimicking thought processes, generating training data, using external tools, and even orchestrating other language models. The tail outcome isn't a better chatbot—it's putting these incredibly well-read, yet unreliable, agents to _work_.\n<br></br>\n<br></br>\n5. This is all made possible by the simple fact that __language functions as a universal API__, a one-size-fits-all interface to digest and understand information. Through language, agents (NOTE: plural!!!!) are capable of planning, navigating, and executing with unprecedented power. Imagine a future where teams of distilled LLMs, each fine-tuned on various downstream tasks, equipped with an array of tools, and orchestrated like seal team fking 6 run on your _local machine_, ready to be dispatched at a moment's notice to plug away to your heart's content—__that future exists now__. \n \n<br></br>\n<br></br> \n\nNOTE: I tried writing this section _so many times_ without feeling like I'd effectively written what I wanted to say. It got to the point where I literally recorded myself spewing for 25 minutes to figure out if anything I said made sense. Language might be powerful, but I clearly haven't learned to wield it. Reading back, I'm still not happy but I can't have another 500 words go to waste.\n \n<br></br>\n<br></br> \n\n$ Miscellanea\n\n- I recently read that for-loops are just syntactic sugar for gotos and map / reduce are just syntactic sugar for for-loops. By analogy, catamorphisms / anamorphisms are just syntactic sugar for folds—holy fk. I still can't manage to wrap my head around recursion schemes, but that analogy took me, like, 5% closer.\n<br></br>\n<br></br>  \n- Step 1 on my [___journey to writing a programming language___](https://www.tjbai.me/-NjOeHhmAZNhjCPp_zex) is complete. Unfortunately, [___the project___](https://github.com/tjbai/p2c) is about 2 tiers above a total dumpster fire, but with the clock ticking down to demo day we're calling it a wrap.\n<br></br>\n<br></br>\n- The team I worked at last summer recently launched their service at [___AWS re:Invent___](https://aws.amazon.com/blogs/security/introducing-iam-access-analyzer-custom-policy-checks/). \n<br></br>\n<br></br>\n\n$s$https://open.spotify.com/track/42TtHTxWk782ZBiqyOi2d1?si=4654f1341e1b4669$d\n\n$s$https://open.spotify.com/track/6Ea2oEzysv4UECGNxL1IEW?si=fdb43e3e3e404104$d\n\n$s$https://open.spotify.com/track/5UWwZ5lm5PKu6eKsHAGxOk?si=e353ccb1ae3c4bb9$d\n\n$s$https://open.spotify.com/track/5PCJY9ByBTQQqP21xDTdjN?si=d2d2ce0044874ba3$d\n\n<br></br>\n<br></br>\n<br></br>\n<br></br>\n<br></br>\n<br></br>",
      "createdDate": "2023-12-18T05:22:19.163Z",
      "labels": [
        "AI",
        "CS"
      ],
      "state": "published",
      "title": "Probably spew"
    },
    "-NmsCDgl8fv0_8uBwCSi": {
      "body": "In this edition:\n- Long-ish spew about nerd shit\n- Short spew about website updates\n\n$br\n\n<br></br>\n<br></br>\n\n$ Reading\n\nMy entire family sleeps at 9 pm, so during family vacations I get a lot of down time to read (and other general time wasting). A smattering of the interesting/foundational papers I've been working through.\n<br></br>\n<br></br>\n- [___GANs___](https://arxiv.org/abs/1406.2661): I'm honestly a little embarassed that it took me this long to learn about generative adversarial networks (GANs). The idea is actually kind of cute—we jointly train one generative MLP to get better at tricking another discriminative one. What's more surprising is the formal guarantees around convergence and how effective they are in practice for sampling generated data. Though they've fallen out of the vogue, it's nice to better understand a formative piece of ML history.\n<br></br>\n<br></br>\n- [___Stable Diffusion___](https://arxiv.org/abs/2112.10752): I haven't had the chance to dive into this as much as I would like because I still don't really understand _why_ the underlying probabilistic diffusion process is the _right_ inductive bias to introduce into image generation. Beyond that, the idea for latent diffusion models (LDMs) to reduce computational complexity by encoding images into a lower-dimensional space seems reasonable and the math all around is pretty cool.\n<br></br>\n<br></br>\n- [___Deepface___](https://arxiv.org/abs/1701.01876): This paper, published in 2014, is a nice reminder of how recently NNs have started to take over as the dominant architecture for learning-enhanced task domains. This reminds me of how I was browsing old CMU machine learning slides a few weeks ago, to study for my final exam, and I stumbled across an old lecture denoting NNs a \"largely impractical novelty.\" \n<br></br>\nPedagogy aside, Deepface is close to my heart because it sits at the core of a project I made freshman year that uses [___facial emotion recognition to recommend music from spotify___](https://github.com/tjbai/new_taste) (I nuked the README and the website so really not much to see anymore). At the time, I was really proud of the novel dimension-reduction function that I used to map emotions onto the [___valence-arousal plane___](https://www.researchgate.net/figure/Valence-Arousal-Plane_fig1_221161945). Looking back, I can only laugh.\n<br></br>\n<br></br>\n- [___LoRA___](https://arxiv.org/abs/2106.09685): I've tried to get through this paper at least a half-dozen times prior, and each time I lose interest or get lost in the first couple of pages. It probably doesn't help that it's been 3+ years since I last had a proper treatment of linear algebra and often forget what the _rank_ of a matrix actually entails, i.e. the dimension of the vector space spanned by the columns.\n<br></br>\nUltimately, a brief discussion of parameter-efficient fine tuning (PEFT) during NLP last semester inspired me to give it a last shot. As it turns out, the math itself is actually embarrassingly simple and it's a bit unclear why I was so daunted before. But, even if I could've understood the method before, lots of the surrounding context would've been lost on me pre-NLP. Cope.\n<br></br>\n<br></br>\n- [___SLAM___](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/Durrant-Whyte_Bailey_SLAM-tutorial-I.pdf): The simultaneous localization and mapping (SLAM) problem is best realized in the setting of self-driving cars. We want some model for our _surroundings_ and our current _location_ based off our observations. This might seem like a chicken-and-egg problem, because having a concrete representation of our surroundings would make it easier to determine our location, whereas knowing our exact location would make it easier to lock-down our surroundings. \n<br></br>\nThe probabilistic SLAM problem is \"solved\" through a simple conditional independence assumption (vehicle state can be modeled as a first-order Markov process), an iterative application of Bayes Theorem, and becomes effective through the observation that errors in mapping our surroundings are largely introduced through errors in our own vehicle state estimates and are thus highly correlated.\n<br></br>\nIt's nice to see \"simple\" math applied in a novel and highly pragmatic fashion. I've made a note to return at some point to better understand implementations such as FastSLAM and EKF-SLAM, likely some point after an upcoming course in stochastic processes and/or monte carlo methods.\n<br></br>\n<br></br>\n\nI'm currently working through the [___Mamba___](https://arxiv.org/abs/2312.00752) paper and trying to understand the surrounding work in _selective state space models_. The math here is definitely _not_ simple, at least in my eyes, but the juice will definitely be worth the squeeze.\n<br></br>\n<br></br>\n\n$ Makeover\n\nI finally got around to making some updates! It's been almost an entire year since I first wrote this website on a sleepless whim. Ever since, the [___github repository's___](https://github.com/tjbai/me) commit history is disappointingly sparse.\n\n<br></br>\n<br></br>\n\nTo my surprise, the code quality was actually semi-decent, and modular enough to make a few changes less than painless:\n<br></br>\n<br></br>\n- __Labels__: I've wanted to add this feature for a while but didn't the diversity of content to justify it. Now, there's an extra layer metadata beyond my often vague titles and one can filter by various topics. If you find the labels displeasing, let me know because I'm not actually in love with the design. There's also a new button on the sidebar to turn them off.\n<br></br>\n<br></br>\n- __Drafts__: I realized that not even _I_ care so much as to have these be visible, and the section was taking up valuable real estate as the number of published posts grew. Most of the drafts also end up being months-old half-baked ideas that I have no intention to complete.\n<br></br>\n<br></br>\n- __Fonts__: This change only makes a difference for Safari users, which I discovered was an upsetting majority. As it turns out, different browser engines have different strategies for rendering __bold__ text. Thus, the visual _aesthetic_ I had carefully curated on Chrome did not translate to non-Chromium-based browsers. The change is simple but I no longer cringe when the page is opened on Sasfari.\n<br></br>\n<br></br>\n\n$ Miscellanea\n- I've been working through [___advent of code___](https://adventofcode.com/2023) this year and have [___earned___](https://github.com/tjbai/aoc) about 40/50 available stars. The problems range from doable for anyone with a pulse to [___pretty much impossible___](https://adventofcode.com/2023/day/21), so ymmv, but I've found it to be a pretty enjoyable way to keep up skills while also learning a few things along the way.\n<br></br>\n<br></br>\n- Will has a [___new blog___](https://substack.com/@williamsunblogs?utm_source=profile-page)! He obviously wants to be me very bad.\n<br></br>\n<br></br>\n- This didn't come out on Christmas because I got lazy and distracted.\n<br></br>\n<br></br>\n<br></br>\n<br></br>",
      "createdDate": "2023-12-30T00:52:04.994Z",
      "labels": [
        "AI",
        "Reading",
        "Meta"
      ],
      "state": "published",
      "title": "Christmas (late)"
    },
    "-Nn5pGoSVZu1OS-AMRsc": {
      "body": "I'm having a bit of an identity crisis with this goofy little website—spoiler alert: the answer is it's _not actually that deep_. Regardless, I can't help but turn a critical lens towards my (relatively) consistent output over the last 2 months.\n\n<br></br>\n<br></br>\n\nSome instinct urges me to put out well-written, novel, and _interesting_ content as some kind of entertainment or social proof to my 0-3 loyal readers. After all, I tend to value other blogs for those same qualities. Unfortunately, I've realized that even acknowledging, let alone embracing, the perception of my writing by some faceless _voyeur_ dramatically hinders its quality, mostly through some combination of insecurity and pretension. \n\n<br></br>\n<br></br>\n\nMostly, I _know_ that I mostly have no idea what I'm talking about, but at the same time I really want other people to believe that I _do_. Thus, I find myself tip-toeing a narrow line between [___wordy garbage of the nonsensical kind (1)___](https://www.tjbai.me/-NlvN-E1ZmY2u8GOtS-x) and [___wordy garbage of the dull kind___](https://www.tjbai.me/-NmsCDgl8fv0_8uBwCSi). \n\n<br></br>\n<br></br>\n\nI've already spoiled the relevant conclusion, so the result of this line of thought is _probably_ that nothing is going to change. Here we go!\n\n<br></br>\n<br></br>\n\n__In today's post:__\n- Medium-length spew about knowledge acquisition\n- Short spew about more nerd shit (reading)\n\n$br\n\n<br></br>\n<br></br>\n\n$ Learning\n\nI really like [___this paper___](https://cacm.acm.org/magazines/2024/1/278891-10-things-software-developers-should-learn-about-learning/fulltext) from the recent _Communications of the ACM_ because it confirms a lot of my preconceived notions (2). \n\n<br></br>\n<br></br>\n\n$\"Understanding a concept goes from abstract to concrete and back\"\n\nYes! I think the corollary to this result is that there are 2 very distinct \"humps\" on the path to _deep_ knowledge acquisition: understanding _what_ and then understanding _why_. Consider websites:\n\n<br></br>\n<br></br>\n\n1. We start with some simple, _a priori_ understandings for how websites work, mostly stemming from our past experiences. We open a _web browser_, type in a _URL_, and then a website shows up that we can read and navigate. At this stage, our knowledge is so shallow that we perceive websites _purely_ as some kind of information/experience that lives at some address on the _internet_—what even is that anyways?\n<br></br>\n2. For those curious enough to try their hand at making websites, they might stumble across various _markdown languages_ like HTML/CSS, _general purpose languages_ like JavaScript, and _frameworks_ like React. Through a combination of YouTube tutorials, it's not difficult to learn that if you type things here and run a command there, you magically have a website that _you_ programmed running on a browser. You've started to understand the _what_. \n<br></br>\n3. What ensues in understanding _why_ is actually a pretty famous interview question—[___what happens when I type google.com in a browser and press enter?___](https://github.com/alex/what-happens-when). Of course the learning at this stage is essentially endless, but crucially the _more_ you understand the _more_ these ideas zoom out from the irrelevant implementation details as specified by languages, runtimes, frameworks, etc. What's important is we once again care about abstractions and might even return to understanding websites simply as a collection of content identified by some domain name, which of course maps to an IPv4/6 address through DNS. \n\n<br></br>\n<br></br>\n\nThe other corollary is that it's often difficult to understand the deep beauty of a subject until you've surpassed that second hump, which anecdotally requires _lots_ of exposure in diverse contexts. This is especially true with mathematics and it's often a turning point for people to realize that math isn't necessarily about numbers and equations, but about logically and formally representing _structure_ and _form_. \n\n<br></br>\n<br></br>\n\nA good example, and a personal watershed moment, is when linear algebra classes (3) teach students from the perspective of [___vector spaces___](https://en.wikipedia.org/wiki/Vector_space) as opposed to systems of equations and matrices. In fact, many universities will often teach with the latter content initially, and then later require math majors to take an upper-level course focused on the former at some point in their degree. Once they've understood the _what_ and have achieved a sufficient level of mathematical maturity, they're expected to understand the _why_.\n\n<br></br>\n<br></br>\n\nIn the same way I may never gain a profound appreciation for biology because all I understand is punnet squares and phylogenetic trees, it sometimes frustrates me that people around me only understand math through their calculus classes or software engineering through React and linked lists. It's even more maddening that there's so much beauty that exists out there, even in my personal domains of interest, that I may never conceive. C'est la vie.\n\n<br></br>\n<br></br>\n\n$\"Experts recognize, beginners reason\"\n\nI wrote something related to this [___last spring in a rant___](https://www.tjbai.me/-NRaZM2LaowSDO0O8T6B) about AMS 430, aka intro stat. Although this is by no means a new idea, the important corollary is that you can directly put yourself on the long path towards expertise by doing things that build your ability to _recognize_. \n\n<br></br>\n<br></br>\n\nFor some of us, that's reading lots of textbooks, doing hundreds of leetcode problems, writing thousands of lines of code, and scrolling Hackernews (I might have a problem). For others, it's the Anki cards and study groups. Whatever works, works.\n\n<br></br>\n<br></br>\n\n$ More Reading\n\nWriting about what I've been reading recently falls under the __\"wordy garbage of the dull kind\"__ category, but whatever.\n<br></br>\n<br></br>\n- [___Towards Monosemanticity___](https://transformer-circuits.pub/2023/monosemantic-features/index.html): Anthropic is out with some really interesting research from their interpretability team. Basically, they train a one-layer transformer and then fit a one-layer, overcomplete autoencoder to predict the activation weights (a form of _sparse dictionary learning_ which I need to learn more about). The result is an intuitive way to understand _what_ exactly different neuron activations represent in context. Cool shit, [___this article___](https://www.astralcodexten.com/p/god-help-us-lets-try-to-understand) is a solid explainer.\n<br></br>\n<br></br>\n- [___Hiring Lemons___](https://danluu.com/hiring-lemons/): I've really been enjoying this guy's blog recently and he has several long essays about issues plaguing talent acquisition in tech (you can see how this functions mainly as personal cope). In this, he refutes a popular argument that because good developers tend to stay put, the job market is disproportionately composed of bad developers. In particular, he argues that companies actually really struggle to identify top performers among their own employees, and then pass on external talent because of various [___cognitive biases___](https://danluu.com/talent/). \n<br></br>\n<br></br>\n- [___Meditations on Moloch___](https://slatestarcodex.com/2014/07/30/meditations-on-moloch/): Every once in a while I fall back down the online rabbit hole of rationalists/EA believers/Eliezer Yudkowsky truthers. I don't really know how to categorize or explain this _incredibly_ long essay, but I think it's _really_ interesting for both better and worse. If you can stomach reading a few thousand words about misaligned economic incentives, transhumanist philosophy, and more then I heavily recommend.\n<br></br>\n<br></br>\n\n__Footnotes__ \n\n(1) [___cocktail party ideas___](https://danluu.com/cocktail-ideas/)\n(2) [___confirmation bias___](https://en.wikipedia.org/wiki/Confirmation_bias#:~:text=Confirmation%20bias%2C%20a%20phrase%20coined,difficult%20to%20dislodge%20once%20affirmed.)\n(3) I can attribute a lot of who I am today to taking Math 217 at UM my senior year of high school.\n\n<br></br>\n\n$s$https://open.spotify.com/track/2PHMNg5e3xMGR0oTLmdj0U?si=4871898280bc49b7$d\n\n$s$https://open.spotify.com/track/57j65yC2HggQfmYNc6rdOK?si=8a99538bf9044f5c$d\n\n$s$https://open.spotify.com/track/76d5duEjc2sorWQFmZsdVC?si=2feef3af31e249e2$d\n\n$s$https://open.spotify.com/track/33iv3wnGMrrDugd7GBso1z?si=344c73a4e2eb4e29$d\n\n<br></br>\n<br></br>\n<br></br>\n<br></br>",
      "createdDate": "2024-01-01T21:01:44.637Z",
      "labels": [
        null,
        "Reading"
      ],
      "state": "published",
      "title": "New Year's Day (on time)"
    },
    "-NnL2c4TrXTAuHBG_5X0": {
      "body": "This is going to be a post-morterm about my recent internship recruiting cycle, so if you're not interested then short circuit! I write this not just as a personal archive but also in case I want to refer anyone here in the future. __(Second pass: this got really long but it's not my worst)__\n\n<br></br>\n<br></br>\n\n__The sparknotes:__\n- a couple dozen OAs \n- interviews with Akuna Capital, Point72, Citadel, Sentry\n- return offer from AWS\n- offer (kind of) from C3.ai\n- offer (real) from Coinbase\n\n<br></br>\n<br></br>\n\n__Genesis:__\n\nAfter securing (and completing) an internship at AWS this last summer, I was originally pretty optimistic about my future career opportunities. After all, how many like me had the experience, the [___nerdy obsession with tech___](https://tjbai.me), and the [___hundreds of finished leetcode problems___](https://leetcode.com/tjbai)? Oh... a lot!\n\n<br></br>\n<br></br>\n\nTo be fair, little did I expect certain, uh, [___tumultuous macroeconomic conditions___](https://www.computerworld.com/article/3685936/tech-layoffs-in-2023-a-timeline.html), although it _is_ a little disingenuous to wholly straw man Jerome Powell. Ultimately, my (mid) prior experience and pedigree afforded me just as many opportunities as I probably deserved. What follows is a story in roughly chronological order.\n\n<br></br>\n<br></br>\n\n__Point72:__\n\nI applied and interviewed for a quant dev position with one of Point72's systematic trading pods in late July. Though not traditionally known for their fantastic tech, I was drawn to the small team environment (READ: profit-shares/head).\n\n<br></br>\n<br></br>\n\nAfter acing an online assessment (2D DP, tricky graph), I was forwarded to a first round with the pod's founding QD (prob/stat fundamentals, design a pricing system, behavioral). I left the interview feeling optimistic and soon thereafter scheduled another call with the head QR. \n\n<br></br>\n<br></br>\n\nTo put it nicely, I got fucking railed. In retrospect, the questions really weren't _that_ difficult (save for one that asked me to derive a big-oh time complexity using exchangeability and approximate convergence properties of the harmonic series). Yet, I recall being _so_ nervous that I even had to hear some questions multiple times. I was flustered by the stakes and got blicked for it.  \n\n<br></br>\n<br></br>\n\nAfter a couple of weeks, I received the dreaded rejection email. Would things had gone differently if this was my _last_ process as opposed to my first? Who knows.\n\n<br></br>\n<br></br>\n\n__Akuna Capital:__\n\nA burgeoning options market-marker based in Chicago, my hubris told me this was a _very_ attainable opportunity. After breezing through a couple online assessment rounds (python trivia, DP-interval problem), I scheduled a penultimate technical round the morning after my dreary Point72 final.\n\n<br></br>\n<br></br>\n\nTo be honest, I don't actually know what went wrong. Aside from the interview, unexpectedly, being entirely _behavioral_, I thought I killed things for the most part. The interviewer even seemed impressed that I actually knew how options _and_ market-makers worked (I know, an incredibly high bar) and told me I would hear back soon. A week later, rejection!\n\n<br></br>\n<br></br>\n\nMy cope tells me that Akuna isn't all that anyways. Who wants to live in the midwest? \n\n<br></br>\n<br></br>\n\n_cope, cope, cope, cope, cope, cope._\n\n<br></br>\n<br></br>\n\n__Citadel:__\n\nOh man. The big daddy. I still remember receiving the first-round invite (after clearing another graph+DP OA) on the MARC train to DC. If a bathroom wasn't in my proximity, I may have shit my pants.\n\n<br></br>\n<br></br>\n\nThis was a _non-negotiable_ opportunity. I pushed my first-round as late as was reasonable and hunkered down for 2 weeks consuming _any-_ and _everything_ I possibly could. Regardless of how things turned out, I still really value all the knowledge I picked up during that short sprint.\n\n<br></br>\n<br></br>\n\nMy first-round was in mid-July during a workday, so around lunch I told my boss I was going to find a \"focus\" area and then snuck off to a spacy conference room. It went well (a few variational LC meds), but I wasn't very confident considering, you know, it's Citadel. After hitting a low in mid-August with L's from Akuna and Point72, the final-round invite hit like a truck. _Oh fuck_. I had 2 more weeks to prepare. I got to work.\n\n<br></br>\n<br></br>\n\nCome interview day, I was emaciated from a long horse-racing weekend and a couple late nights of studying. Yet, I was confident. The Citadel super-day is 3 back-to-back hour-long interviews with _no_ breaks in between. The first interview was _tricky_ but I thought I scraped by with a reasonable answer (don't even know how to explain the problem). The second interview I killed, answering probably 3 LC meds + follow-ups in less than 30 minutes. By the third interview, I had to piss like a firehose and was mentally exhausted, but after an intense hour discussing how to implement a concurrency-safe 'make' utility, I was shaky but hopeful.\n\n<br></br>\n<br></br>\n\nI still haven't heard back :)\n\n<br></br>\n<br></br>\n\n__Sentry:__\n\nAs the summer came to a close, most HFT opportunities came to a close and I found myself disappointingly empty-handed. _It's fine, I'll lock-in for tech companies. It'll be a breeze now, right?_\n\n<br></br>\n<br></br>\n\nApplying for tech internships through September and October was a sobering experience as I discovered the brutal reality of trying to get my resume picked out of a stack of a hundred thousand clones. OAs crept in here and there (always depressingly easy) but interviews didn't. \n\n<br></br>\n<br></br>\n\nSentry, a late-stage cloud observability startup, was one of my only processes during this period. Again, my hubris told me this internship was _easily_ in reach. Who even knows this company? Would I even take it for the sake of resume diversity?\n\n<br></br>\n<br></br>\n\nI'm fairly certain I was Sentry's first ever internship interviewee—if they had designed the interviews to make it as difficult as possible to elucidate _what_ they were even asking me, then I would certainly applaud them. After spending an hour sketching out the architecture for a package manager with associated topo-sort-based dependency resolution for the _nth_ interview, I didn't even care about future steps. I think I finally got rejected a month or so ago, but they wasted so much of my time I didn't even bother reading the email.\n\n<br></br>\n<br></br>\n\n__C3.ai:__\n\nThe fall started to fly by and my luck didn't improve through exams, formals, and Halloweekend. At a point it started to dawn on me that there was a world where AWS didn't give out return offers _and_ I couldn't get anything else. That was a freaky world. Around me I saw lots of other people struggling, but also people with _less_ experience and _less_ nerdy blogs surpassing me. It bittered me that I was clearly playing the game wrong.\n\n<br></br>\n<br></br>\n\nDuring the last week of October the opportunities started to trickle in like molasses, but I finally received an OA invite from C3. To be honest, I didn't (and still don't) have a firm grasp on what C3 really does, but at this point I was ready to take anything that came my way. I had reached Vietnam veteran levels of shell-shock.\n\n<br></br>\n<br></br>\n\nIgnoring the eccentric CEO, rumors of terrible WLB, brutal stock performance, and all around bad vibes, I proceeded with the interview process after clearing the OA (a couple graph problems). The first round was a behavioral with the hiring manager that ended up being surprisingly brutal—_what grade would you give yourself at this past internship?_ _what are __10__ of your weaknesses and __10__ of your strengths?_\n\n<br></br>\n<br></br>\n\nThis probably should've been a red flag but I was ready to drink the kool-aid. The manager was clearly a talented guy and sold me on the idea of working with his team on core ML infrastructure. I swiftly scheduled the next technical round with 2 of the team's engineers. To my surprise, again, I got 2 of the _technically_ most difficult problems that I've ever had (greedy then DP). With just a little help, I cleared them and even got to discuss some nerdy shit (exchange arguments, python core, cache optimization) with the clearly smart interviewers. I was sold.\n\n<br></br>\n<br></br>\n\nWhile I was anxiously waiting for next steps, I heard that Amazon was rolling out a subset of return offers and I cautiously loaded up my job portal. I finally got to breathe a sigh of relief when I saw that my future was _somewhat_ guaranteed, but as always the goal post moved further up. I dreaded the thought of spending another summer in Arlington, VA.\n\n<br></br>\n<br></br>\n\nWhen I finally got an email from the hiring manager extending a verbal offer, I left my NLP lecture and walked outside to accept the sun like a new man. _I was free_. I quickly sent over my information, started to fantasize having a career on the \"cutting edge\" of _ML/AI_, and greedily patrolled my inbox for the official offer letter...\n\n<br></br>\n<br></br>\n\nDays turned to weeks, and weeks turned to months. To this date, despite constant reassurance from HM and recruiters, I still haven't received the paperwork. For a while I gave them the benefit of the doubt—after all, C3 does observe the esoteric practice of having CEO Michael Seibel sign off on _every_ outgoing offer, even interns. But, at a point I started to let it go, and I sank deeper and deeper into the cope. _Amazon's not bad_. _The DMV is up and coming!_ _I'll make L6 in 4 years and show everyone what they're missing_.\n\n<br></br>\n<br></br>\n\n__Coinbase:__\n\nThe cope soon reached a local maximum but there was still an inkling of hope that the C3 offer letter would come through, so I laid my hands to rest and decided to focus on closing out the semester's final exams. From time to time, I would think about the absentee offer letter and use it to hit a new PR. I stopped leetcoding and spent most of my time curled up in bed finishing [___heinous NLP projects___](https://www.tjbai.me/-Nj6FiiK-iTgjZEchZF7).\n\n<br></br>\n<br></br>\n\nOn the Friday after reading period, I was surprised to see an OA invite from Coinbase—I thought I had applied months ago. Picking up my phone, I saw that I _wasn't_ dreaming and that it _was_ actually December. Setting aside my notes, I submitted my CodeSignal score and then burned half an hour on their culture survey, laughing at myself for even letting this get my hopes up. After all, I had submitted probably dozens of perfect OAs by this point just to be ghosted afterwards.\n\n<br></br>\n<br></br>\n\nJust a couple hours later, I got another email in the same thread asking to schedule a first-round phone screen. _Oh?_ I googled [___Coinbase's YTD stock price___](https://finance.yahoo.com/quote/COIN/), then immediately responded with my availability. _Could this be it?_\n\n<br></br>\n<br></br>\n\nThe phone screen was quick and painless and just minutes after I received the final round invite. I scheduled it 2 days later and did just a couple leetcode problems that night to brush up. By then, I had achieved a Master Oogway level of indifference to recruiting—I spent the next 48 hours preparing for a vicious NLP final and only occasionally getting up to drink tea and practice mumbling to myself \"Yeah so I've always been interested in cryptocurrency because...\"\n\n<br></br>\n<br></br>\n\nOn the fateful day, I had been through so many interviews that the nerves totally escaped me. It also helped that I knew Coinbase wasn't known for giving incredibly hard technical questions, rather focusing on software design, best practices, and culture fit. \n\n<br></br>\n<br></br>\n\nMy interviewer joined the Google Meet in a hoodie and I relaxed even more. The problem (and associated followups) wasn't a _total_ joke but lighthearted compared to others I had been through. After about 20 minutes my interviewer declined to ask anymore followups because he had \"gathered enough evidence.\" We then spent the remainder of the hour openly discussing about each other's engineering interests and work—I think I really shined here just because I was _actually_ interested in what he did at Coinbase (plus previous work at Workday, Microsoft) and the questions I asked exposed a lot of my disproportionate (relative to perception) technical erudition.\n\n<br></br>\n<br></br>\n\nI left feeling good, but I knew that didn't mean I was out of the woods. Over the past couple of weeks, I've daydreamed and embarrassing embarrassing amount about living in Mountain View for a summer, getting to _choose_ my intern team, and working with good engineers. Coinbase had a week-long company-wide holiday, so my results were delayed over the new year.\n\n<br></br>\n<br></br>\n\nOn the way back from the beach yesterday, I got a call from an unnamed California area code—__offer secured__. To make matters better, my recruiter sent me the details and benefits within _minutes_ of hopping off the call. \n\n<br></br>\n<br></br>\n\nWhat a long journey.\n\n<br></br>\n<br></br>\n\n__Revelations:__\n\nI'm not going to get ahead of myself and try to wrap this up with some cliche takeaways, but if we extrapolate from the early days of 2024, I might just be a billionaire in a few months. Cheers.\n\n<br></br>\n<br></br>\n\n$s$https://open.spotify.com/track/27F6CgFYMu2nw3uj2RY7uE?si=124c6ba51265499b$d\n\n$s$https://open.spotify.com/track/7t7QJFQLHJWCiHBswHnbzZ?si=a93b993ffbdb4095$d\n\n$s$https://open.spotify.com/track/1FJYX0a18UBzFOWzec43Ih?si=67b9e54b89104892$d\n\n$s$https://open.spotify.com/track/7j3zZ2jAjzFD60UjhldhHo?si=f354a14378824670$d\n\n<br></br>\n<br></br>\n<br></br>\n<br></br>",
      "createdDate": "2024-01-04T19:58:43.982Z",
      "labels": [
        "Career"
      ],
      "state": "published",
      "title": "Seeking Employment"
    },
    "-NnvWNghli6VwjapQ_vz": {
      "body": "I just finished reading [___Decision Transformers___](https://arxiv.org/abs/2106.01345) and am very upset.\n\n<br></br>\n<br></br>\n\nDon't get me wrong—the paper is excellent. The concept is elegant, the methods are thorough, and the writing is so so so coherent.\n\n<br></br>\n<br></br>\n\nAs always with great papers, I wanted to learn a bit more about the authors. _Oh, some Berkeley PhD students? Their future is bright._\n\n<br></br>\n<br></br>\n\nNo. Undergrads.\n\n<br></br>\n<br></br>\n\nFUCK.",
      "createdDate": "2024-01-12T02:34:42.441Z",
      "labels": [
        "Reading",
        "Short"
      ],
      "state": "published",
      "title": "Angered by Excellence"
    },
    "-NoAkcAIVGKvqcTNeAt-": {
      "body": "Working on some random pieces of code for the last week or so—mostly just bored and tinkering with new stuff. Never really get more than few hundred lines deep.\n<br></br>\n<br></br>\n- [___bittorrent___](https://github.com/tjbai/bt/tree/main)\n- [___genetic algos___](https://github.com/tjbai/evo)\n- [___django/htmx and some RAG___](https://github.com/tjbai/dsd_chat)\n\n<br></br>\n<br></br>\n\nRecently invested $20 in a decent mouse (on sale!) and thinking about upgrading to a mechanical keyboard (browns). Unfortunately, this means saying goodbye to my precious trackpad.\n\n<br></br>\n<br></br>\n\n-> No trackpad\n-> Longer distance from keyboard to mouse\n-> Less efficient\n-> Need to finally learn vim \n\n<br></br>\n<br></br>\n\nI've tried learning several times, but keep failing. Finally decided to put my foot down and tape a piece of notebook paper over my trackpad. \n\n<br></br>\n<br></br>\n\n2 hours later, I know vim.\n\n\n\n",
      "createdDate": "2024-01-15T06:15:13.356Z",
      "labels": [
        "Me",
        "CS",
        "Short"
      ],
      "state": "published",
      "title": "Foot. Down."
    },
    "-NoKk_AYcrGxI_JvPwoL": {
      "body": "A couple posts ago I talked about struggling to understanding _why_ diffusion probabilistic models lead to such great results. \n\n<br></br>\n<br></br>\n\nWhile scrolling some professor's research blog, I came across this [___great paper___](https://arxiv.org/abs/2011.13456). The first line in the abstract:\n\n<br></br>\n<br></br>\n\n$\"Creating noise from data is easy; creating data from noise is generative modeling\"\n\nHmmm.\n\n<br></br>\n<br></br>\n\nHaving a principled approach to reading publications is important for resolving knowledge dependencies, especially when learning on the fly. This paper has 2000+ citations. I should have come across it before.\n\n<br></br>\n<br></br>\n\nRecently, reading papers has started to make me really stressed about only having 3 semesters left and _still_ not being involved in research. It's now or never—I know I'd definitely regret not giving it a serious shot during college.\n\n<br></br>\n<br></br>\n\n$s$https://open.spotify.com/track/31bsuKDOzFGzBAoXxtnAJm?si=a23573c9be3b4520$d\n\n<br></br>\n<br></br>\n<br></br>",
      "createdDate": "2024-01-17T04:51:13.248Z",
      "labels": [
        "Reading",
        "Short"
      ],
      "state": "published",
      "title": "Lightbulb"
    },
    "-NpwfHAt1256euudWTVn": {
      "body": "Some quick hit life updates over the past 3 weeks in (rough) chronological order~\n\n<br></br>\n<br></br>\n\n- I built a keyboard that empirically reduces my typing speed from ~150 to ~140. Fortunately, early results indicate it can increase my average typing _satisfaction_ by at least 10-fold.\n\n<br></br>\n<br></br>\n\n- I abandoned building a custom [___Neovim___](https://neovim.io/) configuration in favor of a [___batteries-included distribution___](https://github.com/NvChad/NvChad) with solid defaults. My pride is a little sore, but there's few things more satisfying than an instant IDE just 2 keystrokes away at all times. If anyone knows a good way to run VSCode-style notebooks in Neovim let me know.\n\n<br></br>\n<br></br>\n\n- The infamous [___C3 offer letter___](https://www.tjbai.me/-NnL2c4TrXTAuHBG_5X0) finally came through after a long 2 months. After going back and forth on whether I wanted to pursue a fall co-op during my last year of college, I went with an (un)regretful decline. I was also inbounded for an opportunity with another [___prominent___](https://en.wikipedia.org/wiki/Big_Tech) company, but I'm not entirely enthusiastic about living in Austin, TX.\n\n<br></br>\n<br></br>\n\n- I went through Coinbase's (abbreviated) team matching process and now nervously await the results. There's quite a few teams that I'd really love, but equally many that could make for a blemished summer.\n\n<br></br>\n<br></br>\n\n- Per the end of my [___most recent post___](https://www.tjbai.me/-NoKk_AYcrGxI_JvPwoL), I finally forced myself to write a couple draft emails to PIs in AI/NLP. A couple days later, I was miraculously _inbounded_ for, perhaps, the opportunity of a lifetime. Things to come, but it's been a heady week thinking about [___diffusion models___](https://en.wikipedia.org/wiki/Diffusion_model), [___MCMC___](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo), and more.\n\n<br></br>\n<br></br>\n\n- I started taking an interesting combination of courses spanning AI and statistics. My academic life has slowly devolved into a rigorous indoctrination to [___bayesianism___](https://1000wordphilosophy.com/2022/06/12/bayesianism/). \n\n<br></br>\n<br></br>\n\n- I started my third consecutive semester as a TA (now head) for AMS 420/620 Intro Prob. Though this job pays my dues, there's an unfortunate exponential relationship between x=\"semesters of tenure\" and y=\"embarrassment when I can't answer a question.\" \n\n\n<br></br>\n<br></br>\n\n$s$https://open.spotify.com/track/4MXE6VCvTsQitHWrAxj7Kg?si=9f23e2d753fd46d9$d\n\n$s$https://open.spotify.com/track/4qNYl4NkngYRqf6DtTyD9I?si=849e2b3c3dd04430$d\n\n$s$https://open.spotify.com/track/70YTBH8vOGJNMhy6186yFm?si=432331e6d27741da$d\n\n$s$https://open.spotify.com/track/5QOBT97OmYCZo1W5u7tRrB?si=aff8ca0006f44f7d$d\n\n\n<br></br>\n<br></br>\n<br></br>\n<br></br>\n<br></br>\n<br></br>",
      "createdDate": "2024-02-06T04:28:57.926Z",
      "labels": [
        "Me"
      ],
      "state": "published",
      "title": "Motion"
    },
    "-NslxwEs7KcZeMfuD8F0": {
      "body": "Sleepless 5 a.m. ramblings.\n<br></br>\n<br></br>\n\n$ Setting the stage\n\nI have a French sentence, “Le chien était en colère,” that I want to translate to English. How would I do this?\n\n<br></br>\n<br></br>\n\nLet’s consider 2 families of models: autoregressive and non-autoregressive.\n\n<br></br>\n<br></br>\n\nIn the autoregressive case, we generate our target _sequence_ one token (word) at a time, each time conditioning on a growing left context:\n\n<br></br>\n<br></br>\n\n1. The\n2. The dog\n3. The dog was\n4. The dog was angry\n5. The dog was angry [EOS]\n\n<br></br>\n<br></br>\n\n\"Conditioning\" has its meaning rooted in a probabilistic context, but really all you have to know is that in order to generate the 3rd word, we first have to generate “The dog,” and feed those words into some model that spits out “was” on the other end. Then, we take “The dog was” and put it back in our model to get “angry,” so on and so forth. When we generate the special token EOS (for “end of sentence”), we terminate stop. \n\n<br></br>\n<br></br>\n\nThis should be pretty intuitive. After all, as humans we tend to read and write from left to right, and we tend to use information from the past to inform our decisions in the present. That’s all this _autoregressive_ (“self” - “backwards”) model does. Ignoring, for now, all the complex things that go into training and developing these models, this approach turns out to work pretty well.\n\n<br></br>\n<br></br>\n\nHow can we do better? How can we go _faster_?\n\n<br></br>\n<br></br>\n\nFormally, we develop a soft notion for “how fast is this thing” through something called _asymptotic complexity_—how does the number of operations increase with the size of our desired output sequence? Notice that when we generate the _n_th token in this sequence, we have to _attend over_ (look at) the previous _n-1_ tokens. You can check my math, but when we sum the numbers _1, 2, 3, …, n_, we get an output that’s proportional to n^2. What does this mean? If the correct translation for my input sequence has length _n_, I have to do work to generate that sequence proportional (relative to a constant) to n^2. \n\n<br></br>\n<br></br>\n\nMaybe that’s too slow. Maybe n^2 gets too big as we increase the size of n and we’re stuck waiting longer/using more GPU cycles/increasing our electricity bill more than we want to. What then?\n\n<br></br>\n<br></br>\n\nRather than _condition_ the _n_th token on the previous _n-1_, let’s make the strong assumption that every token in the output sequence can be generated independently. The nice thing about this is since “dog” no longer relies on “The,” we can actually predict both of these tokens at the same time by parallelizing our decoder across the many cores of a GPU.\n\n<br></br>\n<br></br>\n\nIt’s pretty obvious why this is faster. We might even say it takes roughly _constant_ time asymptotically because we could predict a sequence of length $n$ in about the same time as a sequence of length $n+1$, since everything is done in parallel anyways. \n\n<br></br>\n<br></br>\n\nIf this is better, why are most language models autoregressive anyways?\n\n<br></br>\n<br></br>\n\nAs it turns out, it’s pretty important for there to be some level of dependency between tokens when we’re predicting the output sequence. After all, when you’re reading a sentence, does it make sense to look at the 4th word without first looking at the previous 3?\n\n<br></br>\n<br></br>\n\nWhy can’t our model just _learn_ to predict the right tokens at the right positions through the magic of deep learning? This somewhat difficult to answer, but you might already have a strong intuition that language is highly _multimodal_—that is, for every French sentence there’s a large number of _reasonable_ English translations (“The dog was mad,” “The puppy was angry”). Suppose we lived in a world where half the population speaks Yoda English, and instead expects the correct translation to be “Angry the dog was.” What happens if the model gets confused between “The dog was angry” and “Angry the dog was” at different positions in the output and generates “The dog was dog was”? Without _dependency_ between the tokens, this is a pretty hard problem to solve.\n\n<br></br>\n<br></br>\n\nTranslation is just one example of a sequence-to-sequence task—given an input sequence, produce some output sequence. Other familiar tasks might include text summarization (Explain this blog post in fewer words) or question answering (What is the self-attention mechanism in neural networks?). In fact, many tasks can be reformulated as some kind of conditional sequence generation, even ones that aren’t strictly related to text. Given all these pixels, what is the caption for this image? Given this robot in a maze, what sequence of moves should it make to escape?\n\n<br></br>\n<br></br>\n\nAs such, it becomes a problem of great interest to develop general purpose solutions to these kinds of problems that are _both_ accurate _and_ fast.\n\n<br></br>\n<br></br>\n\nOf course, this problem is already pretty well-studied and state-of-the-art approaches lie all over the complex spectrum between fully autoregressive and fully non-autoregressive models. \n\n<br></br>\n<br></br>\n\nOne popular idea is to spread out generation of the large sequence over _many_ NAR steps, each one refining and expanding on the previous step. This is creatively called “iterative refinement.” \n\n<br></br>\n<br></br>\n\nThis should also feel natural, since even humans often struggle to write coherent and correct things on their very first try (all the typos in this blog post are great evidence). Of course, there are possibly deeper reasons to explain why iterative refinement ends up producing superior results to doing everything all at once (something… weight sharing, transfer learning?). But, at a very basic level we can rationalize that even our _errors_ in previous steps might give us important information to generate our current step.\n\n<br></br>\n<br></br>\n\nNow, an abrupt change of topic.\n\n<br></br>\n<br></br>\n\nDiffusion models have emerged in recent years as a dominant generative modeling paradigm, responsible for some of the most incredible and breathtaking foundation image/video models such as Stable Diffusion or OpenAI’s Sora and DALL-E. \n\n<br></br>\n<br></br>\n\nTo give a brief (READ: _very_ hand-wavy) description:\n1. Collect a bunch of examples of the things we want to generate\n2. Corrupt them (apply noising functions) until they’re unrecognizable and contain no information\n3. Learn a reverse model to iteratively convert (denoise) garbage back into its original form\n\n<br></br>\n<br></br>\n\nAs I’ve quoted before on this blog, “Creating noise from data is easy; creating data from noise is generative modeling.”\n\n<br></br>\n<br></br>\n\nDiffusion models are popular in _continuous_ domains such as images because we can use Gaussian noising kernels that have nice theoretical properties (this is actually a bit untrue since many other options, even deterministic ones, can be suitable). In comparison, text is _discrete_. We either predict “dog” or “angry”—there’s nothing in between. Yet, something about the process outlined above should look familiar—doesn't step 3 look an awful lot like _iterative refinement_?\n\n<br></br>\n<br></br>\n\n$ Pulling back the curtain\n\nTo keep this post reasonably short, my research advisor, Prof. Jason Eisner, and I are working on new methods and neural architectures for efficient non-autoregressive generation, inspired by diffusion models.\n\n<br></br>\n<br></br>\n\nI wish there was a nice concise way to describe all the things I've worked on for the past ~4.5 weeks, but in all honesty it's sort of been all over the place. So, for now, at best I will provide the following list of \"directions.\"\n\n<br></br>\n<br></br>\n\n__Pre-trained LMs as noising functions__\n\nTo make the analogy with diffusion models tight, we need to design a robust noising function that \"removes information\" from text sequences. Although previous approaches rely on BERT-style masking, one idea is to just prompt LLMs to do this for us, taking advantage of their semantics to create noising \"trajectories\" in a _smart_ way.\n\n<br></br>\n<br></br>\n\n1. The energy package is hugely important for transparency and a functioning market.\n2. The energy package is important for transparency and a functioning market.\n3. The energy package is important for a functioning market.\n4. The energy package is significant in a market.\n5. The energy package is significant.\n6. The package matters. \n\n<br></br>\n<br></br>\n\nOur goal would then be to train a model that can take \"The package matters\" and produce the following denoising trajectory, optionally conditioned on some context.\n\n<br></br>\n<br></br>\n\n6. The package matters. \n5. The energy package is significant.\n4. The energy package is significant in a market.\n3. The energy package is important for a functioning market.\n2. The energy package is important for transparency and a \n1. The energy package is hugely important for transparency and a functioning market.\n\n<br></br>\n<br></br>\n\n__Imitation learning__\n\nOnce we have these trajectories, how do we learn to go from step 6 to 5, then 4, then 3...? Regardless of the model's underlying architecture, this becomes an _imitation learning_ problem—how do we teach a machine some task by mimicking the behavior of others?\n\n<br></br>\n<br></br>\n\nWhy can't we just treat this as any regular _supervised learning_ problem, and simply train/fine-tune a model to assign high probability to sequence 5 given sequence 6? Well, we actually tried that and the results were nontrivially worse than the baseline. After some analysis, we concluded that our fine-tuned model suffers an _exposure bias_ problem.\n\n<br></br>\n<br></br>\n\nSuppose you were taught math through memorization. Every time you saw 2+2 you were told, and eventually remembered, that the answer was 4. What would happen if you then saw the sum 2+3? Maybe you had also memorized that 3+3 was 6, so you guessed that 2+3 was maybe 5ish? But then what's 5ish+7, 13ishish? And 13ishish-8 is 5ishishish?\n\n<br></br>\n<br></br>\n\nOver time, these errors compound until we're totally past the point of no return, so we clearly need a smarter way to train the model beyond naive supervision: reinforcement learning! The rest of this section probably merits its own post at some point, so I'll leave it there or now.\n\n<br></br>\n<br></br>\n\n__Architectural variants__\n\nRegardless of what you know about them, you've almost certainly heard of neural networks and deep learning. Under the hood, every neural network is a bunch of variables related by equations. \n\n<br></br>\n<br></br>\n\nTo say that we're designing a novel neural architecture just means we're rewriting those equations to generate things in a new way. To say that this system is based on edit-based reconstruction just means that instead of predicting actual _tokens_, we generate a sequence of operations that process some input sequence.\n\n<br></br>\n<br></br>\n\nTo go from \"A cat I saw\" to \"I have seen a dog,\" a reasonable sequence of operations might be:\n\n<br></br>\n<br></br>\n\n1. COPY(2): I\n2. INSERT(\"have\"): I have\n3. SUBSTITUTE(\"saw\", \"seen\"): I have seen\n4. COPY(1): I have seen a\n5. SUBSTITUTE(\"cat\", \"dog\"): I have seen a dog.\n\n<br></br>\n<br></br>\n\nWhy do this rather than directly generate the desired output?\n\n<br></br>\n<br></br>\n\nIt's 5:20 a.m. and my caffeine pills finally wore off. Good night.\n\n<br></br>\n<br></br>\n<br></br>\n<br></br>\n<br></br>\n<br></br>",
      "createdDate": "2024-03-12T09:21:44.793Z",
      "labels": [
        "Research"
      ],
      "state": "published",
      "title": "Research Log"
    },
    "-Nsv4cWRjv_3Vji-62V0": {
      "body": "Without fail, I turn to this blog in times of desperate procrastination.\n\n<br></br>\n<br></br>\n\nA few things that have occupied my mind for the 2 days that no one else cares enough about for me to tell them during a real conversation:\n\n<br></br>\n<br></br>\n\n1. Who decided that \"chat\" would become the default interface to work with LLMs? Though it's true that language is most widely used as a medium for verbal turn-based communication (and recently text-based due to the internet), one can't help but feel that a _conversation_ is certainly _not_ the most effective means for real-time information exchange—it's just the most accessible.\n<br></br>\nMy bold prediction is that in coming years, teams of programmers/researchers will develop specialized DSLs to more efficiently and declaratively interact with models, something like the React to our current vanilla JavaScript. What we call \"prompt engineering\" now will eventually fade away like the legions of early but moot innovations that came before.\n<br></br>\nOne can only hope that LangChain will become nothing more than the Fortran of this era.\n<br></br>\n<br></br>\n2. I've come to the realization that my 2 exams tomorrow are my last ones for the _rest of the semester_. No finals! Instead, 4/4 of my classes have opted for a final project—exciting. \n<br></br>\n    - In my bayesian cognitive modeling and inference class,  neuro-symbolic models for handwritten glyph recognition. \n<br></br>\n    - In my self-supervised (large language) models class—\"deep encoder/shallow encoder\" neural architectures, with a sprinkle of non-autoregressive generation time-permitting.\n<br></br>\n    - In my bayesian statistics class—latent Dirichlet allocation (LDA) for natural language topic modeling (translation: what are these words _about_?)\n<br></br>\n    - In my learning and control theory class—we don't actually get to choose :/ but something about optimal control in biological systems.\n<br></br>\n3. In a [___a previous___](https://www.tjbai.me/-NpwfHAt1256euudWTVn) post, I mentioned that I had recently begun another interview process, one that ended up lasting over a month. After agonizing for a couple days (in a totally privileged, narcissistic way), I fear that I will likely be taking my talents to the great state of Texas this summer. In other news, short COIN and long AAPL.\n<br></br>\n<br></br>\n<br></br>\n<br></br>\n<br></br>",
      "createdDate": "2024-03-14T03:51:56.207Z",
      "labels": [
        "Me",
        "AI"
      ],
      "state": "published",
      "title": "2 Exams Tomorrow"
    },
    "-NuCO_Qjleknu0Jm8cOk": {
      "body": "Missed my bus to NYC.\n\n<br></br>\n<br></br>\n\nStore. Compute. Transfer.\n\n<br></br>\n<br></br>\n\nThat is all.",
      "createdDate": "2024-03-30T03:27:58.200Z",
      "labels": [
        "Me"
      ],
      "state": "published",
      "title": "Mental Model"
    },
    "-NuRm3MOxQjLC847Evvl": {
      "body": "I love idiomatic python:\n\n<br></br>\n<br></br>\n\n```\ndef parse(seq):\n    return ' '.join(seq.split(' ')[1:])\n    \ndef annotate(roll_in):\n    return '\\n\\n'.join(f'{i+1}. {seq}' for i, seq in enumerate(roll_in))\n    \n*trajectory, target = list(map(parse, messages[2]['content'].split('\\n\\n')))\n```\n\n<br></br>\n<br></br>\n\nBut, this is just beautiful:\n\n<br></br>\n<br></br>\n\n```\nlet parse seq = seq |> split \" \" |> drop 1 |> join \" \"\n\nlet annotate roll_in =\n    roll_in\n    |> mapi (fun i seq -> sprintf \"%d. %s\" (i+1) seq)\n    |> join \"\\n\\n\"\n\nlet trajectory, target =\n    match\n    (content\n      |> split \"\\n\\n\"\n      |> map parse\n      |> reverse)\n    with hd, tl -> reverse tl, hd\n``` \n",
      "createdDate": "2024-04-02T03:09:14.608Z",
      "labels": [
        "CS",
        "PL"
      ],
      "state": "published",
      "title": "Beauty"
    },
    "-NukgQdixSUjrJg9ZGDe": {
      "body": "I presented a really quality paper at reading group this week that [___poses and analyzes a hypothesis for the origin of chain-of-thought reasoning in language models.___](https://arxiv.org/abs/2304.03843) During reading group (and later in the day at lab meeting), we ended up discussing a bunch of interesting ideas blending cognitive science and NLP: probing for implicit reasoning in LLMs, calibration/confidence estimation, curriculum learning, etc. Some of these ideas have continued to dwell in my head for a few days now.\n\n<br></br>\n<br></br>\n\nConcurrently this week, famous psychologist David Kahneman, known for authoring _Thinking, Fast and Slow_, passed away. Though I've yet to read his work, his ideas about System 1 vs. System 2 thinking recently entered my context window and has motivated a lot of my reading for the last few.\n\n<br></br>\n<br></br>\n\nJust now, after playing a chess game against Norm, I was thinking about how I would motivate and explain rapid advancements in chess engines over the last decade to someone without any prior knowledge.\n\n<br></br>\n<br></br>\n\nAll these indirectly connected threads then converged in the shower to a really simple appreciation for the idea that _intelligence = recall + reasoning_. Elegant.",
      "createdDate": "2024-04-05T23:57:01.409Z",
      "labels": [
        "AI",
        "Short"
      ],
      "state": "published",
      "title": "Convergence"
    },
    "-Nv-YA1CDchR1vU3GedH": {
      "body": "Monday evenings have been a breath of fresh air.\n\n<br></br>\n<br></br>\n\nThough once they were just the start of another long week, with the weekends providing a respite from the mess of it all, that dynamic has been oddly inverted ever since initiating progress meetings with my research advisor on Mondays. With the pressure to produce results mounting and mounting as each weekend comes and passes, the following evening is always somewhat cathartic.\n\n<br></br>\n<br></br>\n\nThis Monday, I'm feeling particularly inspired, partially comforted knowing there's an entire horse race between where I am today and the next time I need to pay the piper for slacking off during the week.\n\n<br></br>\n<br></br>\n\nA few things I've been thinking about (this is where most of you should/will stop reading):\n<br></br>\n<br></br>\n- Developing a more complete understanding of control theory and dynamical systems this semester has satisfyingly rounded out my mental topology of _artificial intelligence_—perception, learning, decision-making, reasoning, and inference. This is far from a perfect representation (I wish I had something beautiful like store/compute/transfer), but it's a nice way to describe some of the large, somewhat connected topics that just so happen to tickle my fancy.\n<br></br>\n<br></br>\n- I recently stumbled upon the niche imitation learning (IL) because of some  problems I've encountered in research. At a high level, IL attends to the problem that naive supervised learning (behavioral cloning) can be insufficient because of distribution shift/exposure bias (i.e. test data != train data). I find the resulting algorithms (e.g. DAgger) to be simple and elegant in a way that just hasn't clicked for me yet with the broader field of RL. I'm working my way there.\n<br></br>\n<br></br>\n- Lots of talk on my Twitter feed this week about how LLMs (and perhaps statistical learning as a whole) only achieve interpolation, and not true extrapolation (generalization). I can tell that I'm still far away from having an educated take on this argument, but I'm inching closer.\n<br></br>\n<br></br>\n- Lots of my reading in the past year has been done via questionable pdf files from libgen. I borrowed a book today from the CLSP library that caught my eye, _Neural Systems for Control_, and was reminded that nothing beats a weighty, real, and well-bound book. This text in particular is sort of humorous because it was published in 1997, well before \"neural systems\" really became the vogue. At first I was worried that the ideas would be out-of-date, but it turns out that an occasional stroll through (recent) history can be pretty illuminating.\n<br></br>\n<br></br>\n<br></br>\n<br></br>\n<br></br>\n<br></br>\n<br></br>",
      "createdDate": "2024-04-09T01:50:29.474Z",
      "labels": [
        "School",
        "Me",
        "Research"
      ],
      "state": "published",
      "title": "Monday Evenings"
    },
    "-Nv9laFY5I2Xqh2a2nzX": {
      "body": "I read a tweet with a few thousand likes recently (wish I could find the link) that said something like \"gradient descent shows up in unlikely places,\" citing examples like evolution, recommendation systems, etc. This really bothered me because I think an obvious truth is that _gradient descent_ imitates _life_, not the other way around...\n\n<br></br>\n<br></br>\n\n\"Things\" learn to improve through experience, and they make those improvements through small changes that optimize their reward/loss function, whatever that may be. This isn't a deep idea at all, or at least I thought. The analogy is as old as time, and gradient descent is one of those few things I genuinely think _anyone_ could have invented/discovered if they were in the right place at the right time.\n\n<br></br>\n<br></br>\n\nI took a gradient descent step this week to focus more on the \"designing a novel neural architecture\" aspect of my research project. Writing symbols on paper and just thinking about them is a welcome shift from running endless experiments and writing hacky (but beautiful) python.\n\n<br></br>\n<br></br>\n\nSpeaking of, I am obsessed with the work setup I'm running these days. Fluency in vim, a clacky keyboard, and a terminal-first attitude is my \"zen of code.\"\n\n<br></br>\n<br></br>\n\nI recently made ___tjbai.me___ route to a more employer-friendly page, so this blog now lives at ___blog.tjbai.me___. There's some ugly backwards compatibility to ensure old links still work.\n\n<br></br>\n<br></br>\n\nMusic is back!\n\n<br></br>\n<br></br>\n\n$s$https://open.spotify.com/track/5E30LdtzQTGqRvNd7l6kG5?si=1ed7094b6e8f4664$d\n\n$s$https://open.spotify.com/track/3JlNvjlxIlaQ4iQsnxPGct?si=79b8f835e1cf4720$d\n\n$s$https://open.spotify.com/track/33tYADyL2aZctrvR59K1bQ?si=55945d1ff10c457f$d\n\n\n\n<br></br>\n<br></br>\n<br></br>\n<br></br>",
      "createdDate": "2024-04-11T01:29:43.171Z",
      "labels": [
        "Me"
      ],
      "state": "published",
      "title": "Stream of Thought"
    },
    "-NvdzEwzYPgIB75ZtvX4": {
      "body": "I discovered a \"secret\" library at the CLSP, a cluster of offices in their own wing on the 3rd floor of Hackerman, which is really just 2 book shelves and a collection of books categorized into math, programming, and language—quite a dream.\n\n<br></br>\n<br></br>\n\nYesterday, I picked up a copy of Vapnik's _The Nature of Statistical Learning Theory_ and I've been reading a dozen or so pages after dinner in place of the usual TikTok and rot in bed combo. The combination of great exposition and upright posture does wonders to promote _some_ productive use of time right after meals and helps me stay mentally engaged for evening work.\n\n<br></br>\n<br></br>\n\nI'm almost through with Chapter 2 which describes necessary and sufficient conditions for the consistency of learning machines. __Basically, how do we get machines to generalize from training data__? This chapter challenges the core \"self-evident\" intuitions underlying empirical risk minimization. As it turns out, the undergrad-level explanation that \"empirical risk approximates actual risk by law of large numbers\" is sort of weak.\n\n<br></br>\n<br></br>\n\nFrom the preface, Chapter 2 is the most difficult to read due to the dense mathematics throughout but I've found it pretty reasonable so far, partially thanks to having my trusty Claude 3 Sonnet on the side at all times (I'm a proud pro subscriber). Although, I did manage to elicit a hallucination today in a pretty uncharacteristic but humorous way:\n\n<br></br>\n<br></br>\n\nQ: \"Provide an intuitive explanation for [___VC entropy___](https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory).\"\n\n<br></br>\n<br></br>\n\nA: \"Venture Capital (VC) entropy refers to the inherent uncertainty and unpredictability associated...\"\n\n<br></br>\n<br></br>\n\n???\n\n<br></br>\n<br></br>\n\n$s$https://open.spotify.com/track/3IwadYiQKN7O51JgIH9PBC?si=1aba5566028347cb$d\n\n$s$https://open.spotify.com/track/5fqYqPWlfQhIkcUY3XAJlm?si=772b3a5e1aa0400a$d\n\n<br></br>\n<br></br>\n<br></br>\n<br></br>",
      "createdDate": "2024-04-17T01:57:35.502Z",
      "labels": [
        "Reading"
      ],
      "state": "published",
      "title": "Secret Library"
    },
    "-NxLP7skrOLKa5l7meaz": {
      "body": "I've been working somewhat hard to implement a key portion of my research over the last couple weeks. It's been a bit irritating for several reasons, but above all I'm actually frustrated that I'm even having trouble in the first place. After all, writing code isn't research, it's just writing code.\n\n<br></br>\n<br></br>\n\nThe biggest challenge? One extra dimension.\n\n<br></br>\n<br></br>\n\nAnyone with experience writing any code related to machine learning, especially neural nets, has come to both love and hate _tensors_. Remember those matrices that (maybe) gave you headaches in high school? Now, consider a matrix of matrices, or a matrix of matrix of matrices, or a matrix of matrix of matrix of... you get the idea.\n\n<br></br>\n<br></br>\n\nWhy do tensors matter? Because they allow to express complex distributed calculations in a way that's naturally parallelizable by a GPU. Basically, it makes things fast. So, if I want to train a language model, we can actually get it to process an entire sequence all at once, rather than one. token. at. a. time. In fact, we can get it to process _many_ sequences together as part of _batch_.\n\n<br></br>\n<br></br>\n\nLet's do some counting. How many dimensions are we talking?\n\n<br></br>\n<br></br>\n\n- One dimension for all the sequences in a batch.\n- One dimension for all the indices in a sequence.\n- One dimension for all the tokens we predict at each index.\n\n<br></br>\n<br></br>\n\nOk. 3 dimensions. We can handle that.\n\n<br></br>\n<br></br>\n\nYou may recall from my [___previous post___](https://blog.tjbai.me/-NslxwEs7KcZeMfuD8F0) that I'm also working on a type of language model, so why are the challenges any different? The key point is my model doesn't just generate sequences in one left-to-right pass, but rather over _many_ passes that iteratively refine the previous sequence. We call this a trajectory. For example:\n\n<br></br>\n<br></br>\n\n1. Dog. \n2. Dog walk.\n3. I walked the dog.\n4. My friend and I walked the happy dog.\n\n<br></br>\n<br></br>\n\nNow let's count:\n\n<br></br>\n<br></br>\n\n- One dimension for all the _trajectories_ in a batch.\n- One dimension for all the sequences in a trajectory.\n- One dimension for all the indices in a sequence.\n- One dimension for all the _edits_ we predict at each index.\n\n<br></br>\n<br></br>\n\nHmm. 4 dimensions. Fuck. I'm mostly making a big deal out of nothing, but it just so happens that _one_ extra dimension has been creating a world of pain. It doesn't help that basically all reasonable intuition for complex tensor operations breaks down after 3 dimensions. I'll figure it out.\n\n<br></br>\n<br></br>\n\n_P.S. A compiler between imperative for-loops in high-level languages like Python into concise tensor products would be game-changing. Spiritually, this is already what most compilers and tensor frameworks do, just at a totally different level of abstraction._\n\n<br></br>\n<br></br>\n\nFor the title of this post: [___the curse of dimensionality___](https://en.wikipedia.org/wiki/Curse_of_dimensionality#:~:text=The%20curse%20of%20dimensionality%20refers,physical%20space%20of%20everyday%20experience.).\n\n<br></br>\n<br></br>\n\n$s$https://open.spotify.com/track/04CyMEHliadfQWMUJb1w99?si=27004df9eddc45f2$d\n\n$s$https://open.spotify.com/track/37anbFIsjFBmWeXCNGALj3?si=0b482d246942496c$d\n\n$s$https://open.spotify.com/track/7ClS4FdCz9waBZ68pgi1Nf?si=f947c682cc5c43b8$d\n\n$s$https://open.spotify.com/track/5BrTUo0xP1wKXLJWUaGFtk?si=674a83a6e22842eb$d\n\n<br></br>\n<br></br>\n<br></br>\n<br></br>\n<br></br>\n<br></br>",
      "createdDate": "2024-05-08T04:14:03.847Z",
      "labels": [
        "Research",
        "CS"
      ],
      "state": "published",
      "title": "Curses and Dimensions"
    },
    "-NyxjSDhSQeWNQL_7VOm": {
      "body": "In the last few months, I've read the original VAE paper _dozens_ of times, skimmed every blog post and lecture note in existence, and even implemented a hierarchical VAE as the final project for one of my spring courses. Just now, at 12:30 AM in my 85 degree rented shoebox in Austin, I _finally_ understood them. I don't just \"know\" VAEs, I _get_ them. \n\n<br></br>\n<br></br>\n\nThe beauty of finally _getting it_ is that I've also mentally unblocked myself from so many other things—broader variational Bayesian methods, distant connections to expectation-maximization, state-of-the-art VQ-VAEs, etc. \n\n<br></br>\n<br></br>\n\nIf I've achieved nothing else from the last several months of research and mentorship under Prof. Eisner, I've at least developed a great appreciation for the deep deep probabilist's toolkit. I can't claim to have grokked these _big_ ideas quite as much, but my mind echoes of MCMC, EBMs, graphical models, and more.\n\n<br></br>\n<br></br>\n\nP.S. I've been [___streaming___](https://www.youtube.com/@tjbai9797/streams) my research sessions (or some leetcode when I'm too tired) for the last week or so. The quality is pretty brutal on a couple, but I really got in my bag this past weekend.\n\n<br></br>\n\n$s$https://open.spotify.com/track/7j3zZ2jAjzFD60UjhldhHo?si=8b68632dae224eb3$d\n\n$s$https://open.spotify.com/track/0lP4HYLmvowOKdsQ7CVkuq?si=112bafb12ec849df$d\n\n$s$https://open.spotify.com/track/3xlF6fLhMY16LeBLpTK6Eu?si=c860a4c223704939$d",
      "createdDate": "2024-05-28T05:48:05.416Z",
      "labels": [
        "Research"
      ],
      "state": "published",
      "title": "Grokking VAEs"
    },
    "-O-o5un6JHhQKrBGC6OM": {
      "body": "Research has its ups and downs:\n<br />\n<br />\n\n```\n                                                  can't talk, too busy\n                                                  unlocking the secrets\n                                                    of the universe\n                                                  _____________________\n                                                 /                     \\\n        feeling good!                           /                       \\\n       _______________                         /                         \\\n      /               \\                       /                           \\\n     /                 \\                     /                             \\\n    /                   \\___________________/                               \\\n   /                                                                         \\ \n  /                    i will never accomplish                                \\ \n /                   anything of value in my life                              \\\n\n```\n\n.\n<br />\nThe reward signal is a rare combination of sparse (infrequent) _and_ unguaranteed (low EV). In contrast, many of life's \"challenges\" feed reward in a loop that is either:\n<br />\n<br />\n- Frequent and guaranteed, like eating my own meal prep or cooking\n- Frequent and unguaranteed, like trying to hit a new PR\n- Sparse but guaranteed, like annual PTO and biweekly paychecks\n<br />\n<br />\n\nAfter months of challenging work with few meaningful results, what's the payoff? The correct answer is self-gratification, learning, and assurance that I'm somehow contributing to the greater scientific cause, but in practical life-altering terms it's actually _nothing_. \n<br />\n<br />\n\nWhy do it then? Because I'm betting/hoping that...\n<br />\n<br />\n- ....with enough elbow grease I _will_ eventually steer my way to success, at which point the incredible satisfaction will make it all entirely worthwhile.\n<br />\n<br />\n\n- ...research will indirectly transfer rewards to other parts of my life, like my career, in a meaningful enough way to offset the current costs.\n<br />\n<br />\n\nPlus, as much as I may feel like I suffer, I must admit that I _do_ enjoy myself (just a little bit I swear). Still, with just a dim light at the end of the tunnel, my motivation can sometimes be spurious. Historically, I've dealt with this just fine, but after a few too many lonely nights in this 90 degree rental...\n<br />\n<br />\n\n$s$https://open.spotify.com/track/5kJ4BWZ9Y1qFIwwTbMIxYX?si=45d552f4ba264dac$d\n\n$s$https://open.spotify.com/track/5QLHGv0DfpeXLNFo7SFEy1?si=97eb388aafd84bee$d\n\n$s$https://open.spotify.com/track/6DhBce0r9SUlE0QAWvdq4F?si=ef9c63caa92042c4$d\n\n<br ><br />\n<br />",
      "createdDate": "2024-06-20T05:25:47.282Z",
      "labels": [
        "Research"
      ],
      "state": "published",
      "title": "On Stability"
    },
    "-O7qwmOvWH1yq351y_N5": {
      "body": "For the better part of the past 3 years, college has felt like running a marathon at a 4-minute mile pace. Yes, there were frequent water breaks along the way, but from the very first step the only wind at my back was the thought of reaching mile 26.\n\n<br></br>\n<br></br>\n\nNow, the finish line is in sight and my pace at mile 23 is far beyond what I could've ever imagined. I ran so fast that people even paid me for parts of it and I qualified to run an even better race before reaching the end. I think back to the parts where I was gasping for air and can't believe that I managed to continue on.\n\n<br></br>\n<br></br>\n\nOf course, I'm a little sad that I didn't admire the rolling hills and clear blue skies a bit more. But, above anything, I'm more excited about the miles I've yet to see. Do I slow down this final stretch, or go even faster?\n<br></br>\n<br></br>\n\n$s$https://open.spotify.com/track/467t3218y3yihFcyDGWjr0?si=b299564e03c04e2e$d\n\n$s$https://open.spotify.com/track/0U3u0HdRQjD8mAMYBak4Ok?si=cb15103f5bf64b9b$d\n\n$s$https://open.spotify.com/track/3kg7A13eqXpZnnyXEVcGsj?si=505f8af5c3d94c9c$d",
      "createdDate": "2024-09-28T04:45:57.701Z",
      "labels": [
        "Me"
      ],
      "state": "published",
      "title": "Cooling Down"
    },
    "-OB9QQyc01Y3uS-5adzT": {
      "body": "We used to think building AI meant recreating the apparently complexity of human intelligence. Intricate system with specialized modules. Reasoning engines with carefully engineered rules. Sutton's \"bitter lesson\" has always prevailed—when we strip everything away and scale simple approaches, they win. Neural nets beat expert systems. End-to-end training beats fully connected pipelines. Pixel-to-pixel beats APIs.\n\n<br></br>\n<br></br>\n\nComplex behaviors emerge from simple rules. Ant colonies coordinate through basic pheremone signals. Markets reach equilibria through thousands of instantaneous price adjustments. Theory warns us against complexity—Occam's razor, VC dimension, minimum description length. Scaling up fundamentals inevitably uncovers complex behavior.\n\n<br></br>\n<br></br>\n\nSystems find elegant solutions to simple, local objectives. Just get through the next morning, the next evening run, the next weekend. The hardest problems solve themselves when you stop trying to see the whole path forward.\n\n<br></br>\n<br></br>\n\n$s$https://open.spotify.com/track/3BqvSMpXIXHuLUFQgXL4K5?si=ee9894b05664466d$d\n\n$s$https://open.spotify.com/track/4Mve2PSQqMtVn5DemVoBYr?si=8696a31543b64c9d$d",
      "createdDate": "2024-11-08T06:19:44.921Z",
      "labels": [
        "Me"
      ],
      "state": "published",
      "title": "Complexity"
    },
    "-OBiH4YR3cBHJfvd-Z2_": {
      "body": "Compute contention on rockfish (Hopkins GPU cluster) can be pretty intense at times. Every once in a while, I find an quiet window where quick cycles are up for grabs and feel the need to capitalize. My final CV project involves a compute-efficient way to attack VLMs. I sat at my desk close to nonstop from ~3pm until just a few minutes ago. During that time, I was able to improve my model training throughput by close to 50x, burned 30+ A100 hours, and got a couple patches to >90% validation accuracy. \n\n<br></br>\n<br></br>\n\nI hand-rolled a speedy data loader from raw parquet files (torch.utils.data is slop), fucked with the torch profiler to identify bottlenecks, and found opportunities to get massive speedups with cool things like mixed precision and JIT compilation. The biggest step change came from finding some embarassingly obvious parallelization in the image preprocessing pipeline—it evaded me until I was able to reason that the objective would converge even if I sampled the same distortions for an _entire_ batch. Training also converged significantly faster when rather than _clamping_ image parameters to [0,1], I left them unbounded and lazily applied a sigmoid threshold.\n\n<br></br>\n<br></br>\n\nI'm going to miss these days and this kind of access to compute when I graduate. I recently got access to a whole new compute cluster with ~7000 A100s—a top 10 supercomputer just a few years ago.\n\n<br></br>\n<br></br>\n\n$s$https://open.spotify.com/playlist/2grGBgmXMCZnMl4hOYzFWn?si=6333ca0c61a44d5f$d",
      "createdDate": "2024-11-15T05:25:13.516Z",
      "labels": [
        "CS",
        "Me"
      ],
      "state": "published",
      "title": "Zen(d)"
    }
  }
}
